{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":65711,"databundleVersionId":7405009,"sourceType":"competition"},{"sourceId":5536933,"sourceType":"datasetVersion","datasetId":3191230}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1 - Introduction\n\nWelcome to the Playground Season 4, Episode 1! A new year, a new set of challenges! In this particular challenge, we are tasked with predicting whether a user will close their account, or keep it open. Closing an account is known as _churn_. As with many other challenges we've seen previously, the metric we will be using is AUC ROC (Area Under the ROC Curve). The AUC ROC gives us a single metric that tells us how well our classifier is performing, regardless of what our classification threshold is. We'll explore this more in depth a bit later, along with ways we can use ranking to improve our predictions. For now, let's dig into the data!\n\n# 1.1 - Initial Dataset Impressions\n\nLet's look at the memory and disk footprints first, as this can sometimes be a limiting factor on what we can do.\n\n| Dataset | Size on Disk | Size in Memory | # of Rows | # of Cols |\n| ------- | ------------ | -------------- | --------- | --------- |\n| `train` | 7.8 MB       | 45.6 MB        | 165,034   | 14        |\n| `test`  | 12.0 MB      | 21.7 MB        | 110,023   | 13        |\n\nAs we can see, there is a healthy number of rows to work with, suggesting a lot of data for training. This also suggests that we may have better agreement between public and private leaderboards, depending on whether the dataset distributions match each other between train and test. Finally, our memory footprint is not terribly large, although we may start to run into memory pressure issues if we perform too much feature engineering or attempt to build large models.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain = pd.read_csv(\"../input/playground-series-s4e1/train.csv\")\ntest = pd.read_csv(\"../input/playground-series-s4e1/test.csv\")\n\ntrainsize = train.memory_usage(deep=True).sum() / (1024 ** 2)\nprint(f\"train dataset memory usage: {trainsize:,.2f} MB\")\ntrain","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:52:45.512053Z","iopub.execute_input":"2024-01-11T11:52:45.513515Z","iopub.status.idle":"2024-01-11T11:52:48.601453Z","shell.execute_reply.started":"2024-01-11T11:52:45.513468Z","shell.execute_reply":"2024-01-11T11:52:48.599736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testsize = test.memory_usage(deep=True).sum() / (1024 ** 2)\nprint(f\"test dataset memory usage: {testsize:,.2f} MB\")\ntest","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:52:48.604792Z","iopub.execute_input":"2024-01-11T11:52:48.606635Z","iopub.status.idle":"2024-01-11T11:52:48.675552Z","shell.execute_reply.started":"2024-01-11T11:52:48.606574Z","shell.execute_reply":"2024-01-11T11:52:48.673722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Key Observations About Initial Dataset Impressions\n\n* With a large number of samples to learn from, our results may be more stable between local CV, public LB, and private LB. \n* There are 11 different features to learn from:\n    * From a rough overview, they appear to be a mixture of continuous features (`CreditScore`, `Balance`), categorical (`Tenure`, `Surname`, `Geography`), and boolean (`HasCrCard`, `IsActiveMember`). We will need to dig deeper to ensure this is indeed the case.","metadata":{}},{"cell_type":"markdown","source":"# 1.2 - Null Values\n\nLet's explore the issue of missing values in the dataset to see if there are systemic problems with data representation.","metadata":{}},{"cell_type":"code","source":"train[\"null_count\"] = train.isnull().sum(axis=1)\ncounts = train.groupby(\"null_count\")[\"id\"].count().to_dict()\nnull_data = {\"{} Null Value(s)\".format(k) : v for k, v in counts.items() if k < 8}\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 7))\n\naxs = axs.flatten()\n\n_ = axs[0].pie(\n    x=list(null_data.values()), \n    autopct=lambda x: \"{:,.0f} = {:.2f}%\".format(x * sum(null_data.values())/100, x),\n    explode=[0.05] * len(null_data.keys()), \n    labels=null_data.keys(), \n    colors=sns.color_palette(\"Set2\")[0:3],\n)\n_ = axs[0].set_title(\"Number of Null Values Per Row (Train)\", fontsize=15)\n\ntest[\"null_count\"] = test.isnull().sum(axis=1)\ncounts = test.groupby(\"null_count\")[\"id\"].count().to_dict()\nnull_data = {\"{} Null Value(s)\".format(k) : v for k, v in counts.items() if k < 8}\n\n_ = axs[1].pie(\n    x=list(null_data.values()), \n    autopct=lambda x: \"{:,.0f} = {:.2f}%\".format(x * sum(null_data.values())/100, x),\n    explode=[0.05] * len(null_data.keys()), \n    labels=null_data.keys(), \n    colors=sns.color_palette(\"Set1\")[0:3],\n)\n_ = axs[1].set_title(\"Number of Null Values Per Row (Test)\", fontsize=15)\n\ntrain = train.drop(\"null_count\", axis=1)\ntest = test.drop(\"null_count\", axis=1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:52:48.677515Z","iopub.execute_input":"2024-01-11T11:52:48.677870Z","iopub.status.idle":"2024-01-11T11:52:49.066226Z","shell.execute_reply.started":"2024-01-11T11:52:48.677841Z","shell.execute_reply":"2024-01-11T11:52:49.064813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Key Observations About Null Values\n\n* No null values appear in the training or testing datasets.","metadata":{}},{"cell_type":"markdown","source":"# 1.3 - Train / Test Difference - Adversarial Validation\n\nAs a quick test, we should see how different the values are between train and test. To do so, we'll quickly perform a round of adversarial validation to see if a classifier can tell the two datasets apart. We'll use ROC AUC score to inform us of differences. If the two sets appear very similar, the classifier will not be able to tell them apart, and thus will have an ROC AUC score of 0.5. If they are easy to tell apart - and thus are dissimilar - then the ROC AUC score will approach 1. In order to handle categorical values, we'll use simple label encoding.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom lightgbm import LGBMClassifier\nfrom lightgbm import early_stopping, log_evaluation\nfrom sklearn.metrics import roc_curve\nfrom sklearn.preprocessing import LabelEncoder\n\ntrain[\"origin\"] = 0\ntest[\"origin\"] = 1\n\ncombined = train.copy()\ncombined = pd.concat([combined, test]).reset_index(drop=True)\n\nfeatures = [\n    'Surname', 'CreditScore', 'Geography', 'Gender',\n    'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary'\n]\n\nfor feature in [\"Surname\", \"Geography\", \"Gender\"]:\n    le = LabelEncoder()\n    combined[feature] = le.fit_transform(combined[feature])\n    \nn_folds = 5\nskf = KFold(n_splits=n_folds, random_state=2024, shuffle=True)\ntrain_oof_preds = np.zeros((combined.shape[0],))\ntrain_oof_probas = np.zeros((combined.shape[0],))\n\nfor fold, (train_index, test_index) in enumerate(skf.split(combined, combined[\"origin\"])):\n    print(\"-------> Fold {} <--------\".format(fold + 1))\n    x_train, x_valid = pd.DataFrame(combined.iloc[train_index]), pd.DataFrame(combined.iloc[test_index])\n    y_train, y_valid = combined[\"origin\"].iloc[train_index], combined[\"origin\"].iloc[test_index]\n    \n    x_train_features = pd.DataFrame(x_train[features])\n    x_valid_features = pd.DataFrame(x_valid[features])\n\n    model = LGBMClassifier(\n        random_state=2023,\n        objective=\"binary\",\n        metric=\"auc\",\n        n_jobs=-1,\n        n_estimators=2000,\n        verbose=-1,  \n        max_depth=3,\n    )\n    model.fit(\n        x_train_features[features], \n        y_train,\n        eval_set=[(x_valid_features[features], y_valid)],\n        callbacks=[\n            early_stopping(50, verbose=False),\n            log_evaluation(2000),\n        ]\n    )\n    oof_preds = model.predict(x_valid_features[features])\n    oof_probas = model.predict_proba(x_valid_features[features])[:,1]\n    train_oof_preds[test_index] = oof_preds\n    train_oof_probas[test_index] = oof_probas\n    print(\": AUC ROC = {}\".format(roc_auc_score(y_valid, oof_probas)))\n    \nauc_vanilla = roc_auc_score(combined[\"origin\"], train_oof_probas)\nfpr, tpr, _ = roc_curve(combined[\"origin\"], train_oof_probas)\nprint(\"--> Overall results for out of fold predictions\")\nprint(\": AUC ROC = {}\".format(auc_vanilla))\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n\n_ = sns.lineplot(x=[0, 1], y=[0, 1], linestyle=\"--\", label=\"Indistinguishable Datasets\", ax=ax)\n_ = sns.lineplot(x=fpr[::10], y=tpr[::10], ax=ax, label=\"Adversarial Validation Classifier\")\n_ = ax.set_title(\"ROC Curve\", fontsize=15)\n_ = ax.set_xlabel(\"False Positive Rate\")\n_ = ax.set_ylabel(\"True Positive Rate\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:52:49.069892Z","iopub.execute_input":"2024-01-11T11:52:49.072066Z","iopub.status.idle":"2024-01-11T11:53:07.076495Z","shell.execute_reply.started":"2024-01-11T11:52:49.072006Z","shell.execute_reply":"2024-01-11T11:53:07.075123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Key Observations About Train / Test Difference - Adversarial Validation\n\n* The trained classifier has an AUC ROC score of 0.504, which suggests that the training dataset and the testing dataset are very similar.","metadata":{}},{"cell_type":"markdown","source":"# 1.4 - Original Data - Adversarial Validation\n\nOne thing we can also look at is the original dataset, in this case the [Bank Customer Churn Prediction](https://www.kaggle.com/datasets/shubhammeshram579/bank-customer-churn-prediction) dataset. We will perform an adversarial validation to see whether that dataset is similar to the competition dataset.","metadata":{"tags":[]}},{"cell_type":"code","source":"original = pd.read_csv(\"../input/bank-customer-churn-prediction/Churn_Modelling.csv\")\n\ntrain[\"origin\"] = 0\ntest[\"origin\"] = 0\noriginal[\"origin\"] = 1\n\ncombined = train.copy()\ncombined = pd.concat([combined, test]).reset_index(drop=True)\ncombined = pd.concat([combined, original]).reset_index(drop=True)\n\nfeatures = [\n    'Surname', 'CreditScore', 'Geography', 'Gender',\n    'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary'\n]\n\nfor feature in [\"Surname\", \"Geography\", \"Gender\"]:\n    le = LabelEncoder()\n    combined[feature] = le.fit_transform(combined[feature])\n\nn_folds = 5\nskf = KFold(n_splits=n_folds, random_state=2024, shuffle=True)\ntrain_oof_preds = np.zeros((combined.shape[0],))\ntrain_oof_probas = np.zeros((combined.shape[0],))\n\nfor fold, (train_index, test_index) in enumerate(skf.split(combined, combined[\"origin\"])):\n    print(\"-------> Fold {} <--------\".format(fold + 1))\n    x_train, x_valid = pd.DataFrame(combined.iloc[train_index]), pd.DataFrame(combined.iloc[test_index])\n    y_train, y_valid = combined[\"origin\"].iloc[train_index], combined[\"origin\"].iloc[test_index]\n    \n    x_train_features = pd.DataFrame(x_train[features])\n    x_valid_features = pd.DataFrame(x_valid[features])\n\n    model = LGBMClassifier(\n        random_state=2023,\n        objective=\"binary\",\n        metric=\"auc\",\n        n_jobs=-1,\n        n_estimators=2000,\n        verbose=-1,  \n        max_depth=3,\n    )\n    model.fit(\n        x_train_features[features], \n        y_train,\n        eval_set=[(x_valid_features[features], y_valid)],\n        callbacks=[\n            early_stopping(50, verbose=False),\n            log_evaluation(2000),\n        ]\n    )\n    oof_preds = model.predict(x_valid_features[features])\n    oof_probas = model.predict_proba(x_valid_features[features])[:,1]\n    train_oof_preds[test_index] = oof_preds\n    train_oof_probas[test_index] = oof_probas\n    print(\": AUC ROC = {}\".format(roc_auc_score(y_valid, oof_probas)))\n    \nauc_vanilla = roc_auc_score(combined[\"origin\"], train_oof_probas)\nfpr, tpr, _ = roc_curve(combined[\"origin\"], train_oof_probas)\nprint(\"--> Overall results for out of fold predictions\")\nprint(\": AUC ROC = {}\".format(auc_vanilla))\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n\n_ = sns.lineplot(x=[0, 1], y=[0, 1], linestyle=\"--\", label=\"Indistinguishable Datasets\", ax=ax)\n_ = sns.lineplot(x=fpr[::10], y=tpr[::10], ax=ax, label=\"Adversarial Validation Classifier\")\n_ = ax.set_title(\"ROC Curve\", fontsize=15)\n_ = ax.set_xlabel(\"False Positive Rate\")\n_ = ax.set_ylabel(\"True Positive Rate\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:53:07.077693Z","iopub.execute_input":"2024-01-11T11:53:07.078351Z","iopub.status.idle":"2024-01-11T11:54:11.078387Z","shell.execute_reply.started":"2024-01-11T11:53:07.078318Z","shell.execute_reply":"2024-01-11T11:54:11.077375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The AUC ROC score of 0.778 suggests that the original dataset is quite different from the generated one. There may be a number of different reasons, but one obvious one may be due to the names that are used in both datasets. If we remove the names, we can check to see if the remaining data is similar or not.","metadata":{}},{"cell_type":"code","source":"train[\"origin\"] = 0\ntest[\"origin\"] = 0\noriginal[\"origin\"] = 1\n\ncombined = train.copy()\ncombined = pd.concat([combined, test]).reset_index(drop=True)\ncombined = pd.concat([combined, original]).reset_index(drop=True)\n\nfeatures = [\n    'CreditScore', 'Geography', 'Gender',\n    'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary'\n]\n\nfor feature in [\"Surname\", \"Geography\", \"Gender\"]:\n    le = LabelEncoder()\n    combined[feature] = le.fit_transform(combined[feature])\n\nn_folds = 5\nskf = KFold(n_splits=n_folds, random_state=2024, shuffle=True)\ntrain_oof_preds = np.zeros((combined.shape[0],))\ntrain_oof_probas = np.zeros((combined.shape[0],))\n\nfor fold, (train_index, test_index) in enumerate(skf.split(combined, combined[\"origin\"])):\n    print(\"-------> Fold {} <--------\".format(fold + 1))\n    x_train, x_valid = pd.DataFrame(combined.iloc[train_index]), pd.DataFrame(combined.iloc[test_index])\n    y_train, y_valid = combined[\"origin\"].iloc[train_index], combined[\"origin\"].iloc[test_index]\n    \n    x_train_features = pd.DataFrame(x_train[features])\n    x_valid_features = pd.DataFrame(x_valid[features])\n\n    model = LGBMClassifier(\n        random_state=2023,\n        objective=\"binary\",\n        metric=\"auc\",\n        n_jobs=-1,\n        n_estimators=2000,\n        verbose=-1,  \n        max_depth=3,\n    )\n    model.fit(\n        x_train_features[features], \n        y_train,\n        eval_set=[(x_valid_features[features], y_valid)],\n        callbacks=[\n            early_stopping(50, verbose=False),\n            log_evaluation(2000),\n        ]\n    )\n    oof_preds = model.predict(x_valid_features[features])\n    oof_probas = model.predict_proba(x_valid_features[features])[:,1]\n    train_oof_preds[test_index] = oof_preds\n    train_oof_probas[test_index] = oof_probas\n    print(\": AUC ROC = {}\".format(roc_auc_score(y_valid, oof_probas)))\n    \nauc_vanilla = roc_auc_score(combined[\"origin\"], train_oof_probas)\nfpr, tpr, _ = roc_curve(combined[\"origin\"], train_oof_probas)\nprint(\"--> Overall results for out of fold predictions\")\nprint(\": AUC ROC = {}\".format(auc_vanilla))\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n\n_ = sns.lineplot(x=[0, 1], y=[0, 1], linestyle=\"--\", label=\"Indistinguishable Datasets\", ax=ax)\n_ = sns.lineplot(x=fpr[::10], y=tpr[::10], ax=ax, label=\"Adversarial Validation Classifier\")\n_ = ax.set_title(\"ROC Curve\", fontsize=15)\n_ = ax.set_xlabel(\"False Positive Rate\")\n_ = ax.set_ylabel(\"True Positive Rate\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:54:11.080893Z","iopub.execute_input":"2024-01-11T11:54:11.082119Z","iopub.status.idle":"2024-01-11T11:55:00.577923Z","shell.execute_reply.started":"2024-01-11T11:54:11.082001Z","shell.execute_reply":"2024-01-11T11:55:00.576340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Slightly more similar, but still very distinct. Our categorical columns may still be contributing to easy to spot differences. Or, there may be other columns that are contributing to a very easy to spot difference between the original and the synthetic. We should dig in deeper to see if there are very obvious columns that contribute to similarity. If we can rule out easy-to-spot distinctions, we may be able to use the original dataset as a source of training data.","metadata":{}},{"cell_type":"markdown","source":"### Key Observations About Original Data - Adversarial Validation\n\n* The classifier has an AUC ROC score of 0.778 - this is quite high, which suggests there are some easy-to-spot differences between the datasets.\n    * Caution should be used when mixing data.\n    * The `Surname` categorical column appears to be a distinguishing factor between datasets.\n    * We should look to see if there are other big differences between datasets, so we can potentially use the original data to our advantage.","metadata":{}},{"cell_type":"markdown","source":"# 1.5 - Spearman Correlation\n\nWe should check to see if there is high correlation between our features. Spearman correlation does not make assumptions about distribution types or linearity. With Spearman correlation, we have values that range from -1 to +1. Values around either extreme end mean a negative or positive correlation respectively, while those around 0 mean no correlation exists.","metadata":{}},{"cell_type":"code","source":"train_copy = train.copy()\n\nfeatures = [\n    'Surname', 'CreditScore', 'Geography', 'Gender',\n    'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary', 'Exited'\n]\n\nfor feature in [\"Surname\", \"Geography\", \"Gender\"]:\n    le = LabelEncoder()\n    train_copy[feature] = le.fit_transform(train_copy[feature])\n\ncorrelation_matrix = train_copy[features].corr(method=\"spearman\")\n\nfrom matplotlib.colors import SymLogNorm\n\nf, ax = plt.subplots(figsize=(20, 20))\n_ = sns.heatmap(\n    correlation_matrix, \n    mask=np.triu(np.ones_like(correlation_matrix, dtype=bool)), \n    cmap=sns.diverging_palette(230, 20, as_cmap=True), \n    center=0,\n    square=True, \n    linewidths=.1, \n    cbar=False,\n    ax=ax,\n    annot=True,\n)\n_ = ax.set_title(\"Spearman Correlation Matrix\", fontsize=15)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:55:00.579977Z","iopub.execute_input":"2024-01-11T11:55:00.580464Z","iopub.status.idle":"2024-01-11T11:55:01.749436Z","shell.execute_reply.started":"2024-01-11T11:55:00.580427Z","shell.execute_reply":"2024-01-11T11:55:01.746832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Key Observations about Spearman Correlation\n\n* A strong negative correlation exists between the `NumOfProducts` and `Balance`. This is interesting, since it suggests that high balances are not tied to large number of products.\n* A weakly negative correlation exists between `Age` and `NumOfProucts`.\n* A strong positive correlation exists between `Geography` and `Balance`, suggesting there may be areas of the world that have higher balances.\n* A negative correlation exists between `Surname` and `Balance`.\n* In terms of correlations to target:\n    * Strong positive correlations exist between `Age` and `Exited`, as well as `Balance` and `Exited`.\n    * Strong negative correlations exist between `NumOfProducts` and `Exited`, `Gender` and `Exited`, as well as `IsActiveMember` and `Exited`.","metadata":{}},{"cell_type":"markdown","source":"# 1.7 - Statistical Breakdown\n\nLet's take a closer look at some of the statistical properties of the continuous features. As a reminder, what we're looking for between the two sets is big differences between the min, max, and standard deviations for each continuous column. Differences there tell us if there are outliers that we need to deal with. Let's start with the training set.","metadata":{}},{"cell_type":"code","source":"features = [\n    'CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary'\n]\n\ntrain[features].describe().T.style.bar(subset=['mean'], color='#7BCC70')\\\n    .background_gradient(subset=['std'], cmap='Reds')\\\n    .background_gradient(subset=['50%'], cmap='coolwarm')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:55:01.752437Z","iopub.execute_input":"2024-01-11T11:55:01.753035Z","iopub.status.idle":"2024-01-11T11:55:01.926040Z","shell.execute_reply.started":"2024-01-11T11:55:01.752990Z","shell.execute_reply":"2024-01-11T11:55:01.923833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And for the testing set:","metadata":{}},{"cell_type":"code","source":"features = [\n    'CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary'\n]\n\ntest[features].describe().T.style.bar(subset=['mean'], color='#7BCC70')\\\n    .background_gradient(subset=['std'], cmap='Reds')\\\n    .background_gradient(subset=['50%'], cmap='coolwarm')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:55:01.929616Z","iopub.execute_input":"2024-01-11T11:55:01.930062Z","iopub.status.idle":"2024-01-11T11:55:02.007360Z","shell.execute_reply.started":"2024-01-11T11:55:01.930036Z","shell.execute_reply":"2024-01-11T11:55:02.004405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Key Observations about Statistical Breakdown\n\n* The min, max, and standard deviations between datasets suggest they are nearly identical.\n    * This fact is supported by the adversarial validation performed above.\n* Patrons who use the bank tend to have balances that are over $70,000:\n    * This can be observed by the fact that the 25 percentile for estimated salary is 74,440, and the mean is $112,315.\n    * This may suggest that this bank is primarily based in savings and investment.\n* Patrons are mainly grouped around the ages 32 - 42.\n* Most patrons have either one or two products, few have more than two.\n* The average number of years patrons have been with the bank are 2.8. \n    * Rarely do patrons stay more than 7 years.\n* Credit scores look to be fairly evenly distributed, although we should check to make sure there are no oddities.","metadata":{}},{"cell_type":"markdown","source":"# 1.8 - P-Value Testing\n\nWhile looking at features visually will tell us some interesting information, we can also use p-value testing to see if a feature has a net impact on a simple regression model. This method is controversial in that it likely doesn't provide a correct look at what features are informative. Our null hypothesis is that the feature impacts the target variable of `Exited`. In this case, anything with a p-value greater than 0.05 means we reject that hypothesis, and can potentially flag it for removal. Based on our current examination thus far, and given how few features we have to work with, it is very likely that all our features are informative.","metadata":{}},{"cell_type":"code","source":"from statsmodels.regression.linear_model import OLS\nfrom statsmodels.tools.tools import add_constant\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfeatures = [\n    'Surname', 'CreditScore', 'Geography', 'Gender',\n    'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary'\n]\n\ntrain_copy = train.copy()\n\nfor feature in [\"Surname\", \"Geography\", \"Gender\"]:\n    le = LabelEncoder()\n    train_copy[feature] = le.fit_transform(train_copy[feature])\n\nx = add_constant(train_copy[features])\nmodel = OLS(train_copy[\"Exited\"], x).fit()\n\npvalues = pd.DataFrame(model.pvalues)\npvalues.reset_index(inplace=True)\npvalues.rename(columns={0: \"pvalue\", \"index\": \"feature\"}, inplace=True)\npvalues.style.background_gradient(cmap='YlOrRd')","metadata":{"tags":[],"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:55:02.009729Z","iopub.execute_input":"2024-01-11T11:55:02.010304Z","iopub.status.idle":"2024-01-11T11:55:02.462898Z","shell.execute_reply.started":"2024-01-11T11:55:02.010253Z","shell.execute_reply":"2024-01-11T11:55:02.461420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Key Observations about P-Value Test\n\n* P-value testing suggests there are no features that are likely candidates for removal.","metadata":{}},{"cell_type":"markdown","source":"# 1.9 - Duplicated Rows\n\nAs was apparent in past episodes of the Playground, duplicate entries can cause unique challenges to a competition. We should check to see how many duplicate rows exist. In this case, we'll look for duplicates in the training and testing sets, without considering the `id` or `Exited` columns.","metadata":{}},{"cell_type":"code","source":"duplicates = train.pivot_table(index=[\n    'Surname', 'CreditScore', 'Geography', 'Gender',\n    'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary'\n], aggfunc=\"size\")\nunique, counts = np.unique(duplicates, return_counts=True)\nvalue_counts = dict(zip(unique, counts))\n\nif len(unique) == 1:\n    print(\": There are no duplicated rows in the training set\")\nelse:\n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 8))\n\n    _ = sns.barplot(x=list(value_counts.keys())[1:], y=list(value_counts.values())[1:], ax=ax)\n    _ = ax.set_title(\"Duplicate Counts in Training Set\", fontsize=15)\n    _ = ax.set_ylabel(\"Count\")\n    _ = ax.set_xlabel(\"Number of Times Row is Duplicated\")\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(\n            x=p.get_x()+(p.get_width()/2),\n            y=height,\n            s=\"{:d}\".format(int(height)),\n            ha=\"center\"\n        )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:55:02.468318Z","iopub.execute_input":"2024-01-11T11:55:02.469227Z","iopub.status.idle":"2024-01-11T11:55:02.979687Z","shell.execute_reply.started":"2024-01-11T11:55:02.469184Z","shell.execute_reply":"2024-01-11T11:55:02.977598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We should also check for duplicates in the testing set:","metadata":{}},{"cell_type":"code","source":"duplicates = test.pivot_table(index=[\n    'Surname', 'CreditScore', 'Geography', 'Gender',\n    'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary'\n], aggfunc=\"size\")\nunique, counts = np.unique(duplicates, return_counts=True)\nvalue_counts = dict(zip(unique, counts))\n\nif len(unique) == 1:\n    print(\": There are no duplicated rows in the testing set\")\nelse:\n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 8))\n\n    _ = sns.barplot(x=list(value_counts.keys())[1:], y=list(value_counts.values())[1:], ax=ax)\n    _ = ax.set_title(\"Duplicate Counts in Testing Set\", fontsize=15)\n    _ = ax.set_ylabel(\"Count\")\n    _ = ax.set_xlabel(\"Number of Times Row is Duplicated\")\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(\n            x=p.get_x()+(p.get_width()/2),\n            y=height,\n            s=\"{:d}\".format(int(height)),\n            ha=\"center\"\n        )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:55:02.984130Z","iopub.execute_input":"2024-01-11T11:55:02.984830Z","iopub.status.idle":"2024-01-11T11:55:03.449998Z","shell.execute_reply.started":"2024-01-11T11:55:02.984795Z","shell.execute_reply":"2024-01-11T11:55:03.448186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And finally, we should check for duplicates between the training and testing sets.","metadata":{}},{"cell_type":"code","source":"combined = pd.concat([train, test]).reset_index(drop=True)\n\nduplicates = combined.pivot_table(index=[\n    'Surname', 'CreditScore', 'Geography', 'Gender',\n    'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary'\n], aggfunc=\"size\")\nunique, counts = np.unique(duplicates, return_counts=True)\nvalue_counts = dict(zip(unique, counts))\n\nif len(unique) == 1:\n    print(\": There are no duplicated rows in the combined set\")\nelse:\n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 8))\n\n    _ = sns.barplot(x=list(value_counts.keys())[1:], y=list(value_counts.values())[1:], ax=ax)\n    _ = ax.set_title(\"Duplicate Counts in Training and Testing Sets Combined\", fontsize=15)\n    _ = ax.set_ylabel(\"Count\")\n    _ = ax.set_xlabel(\"Number of Times Row is Duplicated\")\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(\n            x=p.get_x()+(p.get_width()/2),\n            y=height,\n            s=\"{:d}\".format(int(height)),\n            ha=\"center\"\n        )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:55:03.452161Z","iopub.execute_input":"2024-01-11T11:55:03.452598Z","iopub.status.idle":"2024-01-11T11:55:04.172366Z","shell.execute_reply.started":"2024-01-11T11:55:03.452572Z","shell.execute_reply":"2024-01-11T11:55:04.170377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is an interesting result, since it demonstrates that there is duplicate column overlap between training and testing in the order of over 170 rows. If this is the case, then we may be able to copy the prediction result directly from the training set to the testing set to boost our classifier performance. If we take a closer look at the rows that are duplicated:","metadata":{}},{"cell_type":"code","source":"train['origin'] = \"train\"\ntest['origin'] = \"test\"\ncombined = pd.concat([train, test]).reset_index(drop=True)\n\nfeatures = [\n    'Surname', 'CreditScore', 'Geography', 'Gender',\n    'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary'\n]\n\npd.set_option('display.max_rows', 70)\ncombined[combined.duplicated(subset=features, keep=False)].sort_values(by=features)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:55:04.177656Z","iopub.execute_input":"2024-01-11T11:55:04.178091Z","iopub.status.idle":"2024-01-11T11:55:04.436566Z","shell.execute_reply.started":"2024-01-11T11:55:04.178060Z","shell.execute_reply":"2024-01-11T11:55:04.435381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Key Observations about Duplicated Rows\n\n* There appear to be training samples that we may have a direct target for in the training set:\n    * Surname of `Ahern` with a credit score of 745 appears in both the training and testing sets.\n    * If we assume the target column of `Exited` is the same for the both, then we can use the training target as our test prediction. \n* There is evidence however, that the duplicate columns may have different mappings to `Exited`:\n    * Surname of `Zetticci` with a credit score of 791 appears in the training set twice, with the only difference being the target value of `Exited`.\n    * This duplicate outcome is likely to confuse our classifier, since one is a positive example, and the other is negative.\n* Depending on how these duplicates were created, if we assume for a moment that these were meant to be distinct rows of data, then we can make the opposite prediction from the training set in the testing set. We can test this empirically by burning a few submissions to see how it impacts score.","metadata":{}},{"cell_type":"markdown","source":"# 1.10 - Dimensionality Reduction - UMAP\n\nWe may want to explore reducing the dimensionality of the data to see if there is anything interesting. We can use UMAP (Uniform Manifold Approximation and Projection) to reduce the dataset into a fewer number of dimensions. Similar to a PCA, UMAP attempts to reduce the dimensionality of a dataset by utilizing manifold learning. It's primary purpose is to help visualize what a dataset looks like. Ideally, this reduction in dimensionality may also be of benefit when it comes to machine learning tasks, in this case, potentially giving rise to a classifier with better performance than one that attempts to use the entire dataset. We can use a UMAP reduction and use the reduced features in a machine learning regressor directly.","metadata":{}},{"cell_type":"code","source":"import umap\n\numap_train = train.copy()\numap_train = umap_train.iloc[::10, :]\n\nfeatures = [\n    'Surname', 'CreditScore', 'Geography', 'Gender',\n    'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary'\n]\n\nfor feature in [\"Surname\", \"Geography\", \"Gender\"]:\n    le = LabelEncoder()\n    umap_train[feature] = le.fit_transform(umap_train[feature])\n\nreducer = umap.UMAP(random_state=2023)\nreduced_data = reducer.fit_transform(umap_train[features])\numap_train[\"2d_x\"] = reduced_data[:, 0]\numap_train[\"2d_y\"] = reduced_data[:, 1]\n\nsns.set(style='white', context='notebook')\n\nf, ax = plt.subplots(figsize=(10, 10))\n\n_ = sns.scatterplot(data=umap_train, x=\"2d_x\", y=\"2d_y\", hue=\"Exited\", ax=ax)\n_ = ax.set_title('2D Reduced Representation', fontsize=24)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:55:04.438171Z","iopub.execute_input":"2024-01-11T11:55:04.438928Z","iopub.status.idle":"2024-01-11T11:56:28.406012Z","shell.execute_reply.started":"2024-01-11T11:55:04.438887Z","shell.execute_reply":"2024-01-11T11:56:28.404428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Key Observations about Dimensionality Reduction\n\n* As we can see from the image, positive and negative samples are fairly intermixed when viewing a reduced representation.\n    * There may be pockets of x and y coordinates that provide separation between classes (e.g. x between 10 and 15 with y between 3 and 5).\n    * The benefit here may not be so certain as a caveat to this approach is that only every 10th row of data was used to produce the above image.","metadata":{}},{"cell_type":"markdown","source":"# 1.11 - Class Imbalance\n\nFinally, we should check to see if there is any significant class skew that may impact our classifier and its performance.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 8))\n\nsns.set_style('darkgrid')\nsns.set_palette('Set2')\n\ncounts = pd.DataFrame({\"Exited\": [train[\"Exited\"].value_counts()[0], train[\"Exited\"].value_counts()[1]]})\n_ = sns.barplot(x=counts.index, y=counts.Exited, ax=axs[0])\nfor p in axs[0].patches:\n    axs[0].text(x=p.get_x()+(p.get_width()/2), y=p.get_height(), s=\"{:,d}\".format(round(p.get_height())), ha=\"center\")\n_ = axs[0].set_title(\"Class Balance\", fontsize=15)\n_ = axs[0].set_ylabel(\"Number of Records\", fontsize=15)\n_ = axs[0].set_xlabel(\"Target\", fontsize=15)\nfor label in axs[0].get_xticklabels():\n    label.set_rotation(45)\n    label.set_ha('right')\n\ntargets = train[\"Exited\"].unique()\ndata = [train[(train[\"Exited\"] == target)][\"id\"].count() for target in targets]\n_ = axs[1].pie(\n    data, labels=targets,\n    autopct=lambda x: \"{:,.0f} = {:.2f}%\".format(x * sum(data)/100, x),\n    explode=[0.20] * len(data), \n    colors=sns.color_palette(\"Set2\")[0:len(data)],\n)\n_ = axs[1].set_title(\"Class Balance\", fontsize=15)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:28.408442Z","iopub.execute_input":"2024-01-11T11:56:28.410320Z","iopub.status.idle":"2024-01-11T11:56:28.902757Z","shell.execute_reply.started":"2024-01-11T11:56:28.410199Z","shell.execute_reply":"2024-01-11T11:56:28.900894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Key Observations about Class Balance\n\n* As we can see, the dataset is heavily skewed to an `Exited` value of 0. \n* Detecting `Exited` values of 1 is likely going to be difficult, as they are rare when compared to the negative class.","metadata":{}},{"cell_type":"markdown","source":"# 2 - Feature Exploration\n\nLet's take a closer look at the features we have in our dataset.\n\n\n# 2.1 - Surname\n\nThe `Surname` feature of the dataset provides the last name of the customer. In terms of usefulness, the Spearman correlation suggested that it was positively correlated with bank balance. Balance in turn was correlated with our target variable. A combination of these features may provide us some insights. First, let's look to see how many unique last names we are dealing with.","metadata":{}},{"cell_type":"code","source":"print(f\": Number of unique surnames = {train['Surname'].nunique():,d}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:28.904740Z","iopub.execute_input":"2024-01-11T11:56:28.905129Z","iopub.status.idle":"2024-01-11T11:56:28.925058Z","shell.execute_reply.started":"2024-01-11T11:56:28.905103Z","shell.execute_reply":"2024-01-11T11:56:28.923560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is interesting given there are 165,034 rows of data. This means there is a lot of overlap between last name and number of records. Let's take a look at the top 24 surnames and see if there are any that stick out as having higher correlation to the target.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=6, ncols=4, figsize=(15, 25))\n\nsns.set_style('darkgrid')\nsns.set_palette('Set2')\n\ntop_surnames = [surname for surname in train[\"Surname\"].value_counts().nlargest(24).index]\nlabels = top_surnames.copy()\naxs = axs.flatten()\n\nfor ag, surname in enumerate(top_surnames):\n    data = [\n        train[(train[\"Exited\"] == 0) & (train[\"Surname\"] == surname)][\"id\"].count(),\n        train[(train[\"Exited\"] == 1) & (train[\"Surname\"] == surname)][\"id\"].count()\n    ]\n\n    label = [\"Did Not Exit\", \"Exited\"]\n    _ = axs[ag].pie(\n        data, labels=label,\n        autopct=lambda x: \"{:,.0f} = {:.2f}%\".format(x * sum(data)/100, x),\n        explode=[0.05] * 2, \n        pctdistance=0.5, \n        colors=sns.color_palette(\"Set2\")[0:2],\n    )\n    _ = axs[ag].set_title(f\"{labels[ag]}\", fontsize=15)\n    \naxs[5].set_axis_off()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:28.926973Z","iopub.execute_input":"2024-01-11T11:56:28.927394Z","iopub.status.idle":"2024-01-11T11:56:32.039968Z","shell.execute_reply.started":"2024-01-11T11:56:28.927363Z","shell.execute_reply":"2024-01-11T11:56:32.037937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, there are no magic bullets. Some notable surnames such as `Genovese` and `Ch'ang` have slightly higher `Exited` occurrences than their counterparts, while `Onyemauchechukwu` has a much smaller number of `Exited` occurrences. Gradient boosted decision trees may be able to exploit this if we make use of label encoding. \n\nStrangely though, if we think about surnames and their impact on whether someone leaves the bank, we would expect name to have no impact on the actual result. In other words, in our pie chart, we would expect the different surnames to exit the bank at roughly the same proportions as each other, but clearly this is not the case (for example with `Genovese`). If we examine the surnames closer as well, we find some oddities, such as `H?` and `Hs?`. We may be seeing artifacts of the synthetic data generation scheme at work. What happens if we roll up surnames to first letter only and look at their breakdown of exiting the bank?","metadata":{}},{"cell_type":"code","source":"train[\"Surname_First_Letter\"] = train[\"Surname\"].apply(lambda x: x[0])\n\nfig, axs = plt.subplots(nrows=5, ncols=5, figsize=(20, 25))\n\nsns.set_style('darkgrid')\nsns.set_palette('Set2')\n\ntop_surnames = [surname for surname in train[\"Surname_First_Letter\"].value_counts().index]\nlabels = top_surnames.copy()\naxs = axs.flatten()\n\nfor ag, surname in enumerate(top_surnames):\n    data = [\n        train[(train[\"Exited\"] == 0) & (train[\"Surname_First_Letter\"] == surname)][\"id\"].count(),\n        train[(train[\"Exited\"] == 1) & (train[\"Surname_First_Letter\"] == surname)][\"id\"].count()\n    ]\n\n    label = [\"Did Not Exit\", \"Exited\"]\n    _ = axs[ag].pie(\n        data, labels=label,\n        autopct=lambda x: \"{:,.0f} = {:.2f}%\".format(x * sum(data)/100, x),\n        explode=[0.05] * 2, \n        pctdistance=0.5, \n        colors=sns.color_palette(\"Set2\")[0:2],\n    )\n    _ = axs[ag].set_title(f\"{labels[ag]}\", fontsize=15)\n    \naxs[5].set_axis_off()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:32.042816Z","iopub.execute_input":"2024-01-11T11:56:32.043586Z","iopub.status.idle":"2024-01-11T11:56:35.490882Z","shell.execute_reply.started":"2024-01-11T11:56:32.043524Z","shell.execute_reply":"2024-01-11T11:56:35.489777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, we can see that if the surname starts with `Q`, they are much more likely to leave the bank (however, this can partly be explained by the small numbers of samples in those groups). But if we look at surnames that start with `T` and surnames that start with `C`, we can see a stark difference between those exit numbers. We may want to experiment with first letter surnames as a feature in the classifier to see if it provides lift.\n\n### Key Observations about Surname\n\n* Surname field alone is likely not going to be too helpful in generating lift for our classifier.\n* We may be able to combine Surname with other fields to see if there is benefit.\n* Different types of categorical encoding may make a difference to our classifier.\n* Using only the first letter in the surname as a feature may provide lift.","metadata":{}},{"cell_type":"markdown","source":"# 2.2 - CreditScore\n\nThe `CreditScore` feature of the data appears to correlate with the patron's real world credit score. According to [Investopedia](https://www.investopedia.com/terms/c/credit_score.asp) a credit score is used as a general guideline to determine whether an individual is a good credit risk (i.e. if they should be given financial credit). The score ranges in values between 300 and 850. Scoring higher means that an individual will likely have a better chance at securing various forms and amounts of credit. Usually those with lower values of credit scores are more likely to make payments late, owe larger amounts, have shorter credit histories, or have too many other lines of credit open.\n\nIn terms of relating a credit score to churn, our Spearman correlation showed us that the raw score did not appear to have a strong positive or negative correlation to the target variable. However, there may be ways of segmenting or otherwise modifying the credit score to be more informative. Let's look first at some density estimates to see where the bulk of our credit scores are located.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))\n\n_ = sns.kdeplot(train[\"CreditScore\"], shade=True, color=\"r\", ax=axs, label=\"Credit Score Densities\")\n_ = axs.set_title(\"Credit Score Densities\", fontsize=15)\n_ = axs.set_ylabel(\"Density\")\n_ = axs.set_xlabel(\"Credit Score\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:35.492177Z","iopub.execute_input":"2024-01-11T11:56:35.492502Z","iopub.status.idle":"2024-01-11T11:56:36.485964Z","shell.execute_reply.started":"2024-01-11T11:56:35.492474Z","shell.execute_reply":"2024-01-11T11:56:36.484692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As was indicated on our statistical breakdown above, the bulk of our scores occur around the 650 mark, and tend not to stray too much below 580. This provides us a unique insight based on Investopedia, where credit scores can be categorized into good versus poor risks for credit:\n\n* 300 - 579 = Poor\n* 580 - 669 = Fair\n* 670 - 739 = Good\n* 740 - 799 = Very Good\n* 800 - 850 = Excellent\n\nThis suggests we may be able to create a new column that segments patrons into different credit ranges, and use that as a feature. Let's look at whether this will be helpful.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(10, 15))\n\nsns.set_style('darkgrid')\nsns.set_palette('Set2')\n\ncredit_score_ranges = [(300, 579), (579, 669), (669, 739), (739, 799), (799, 950)]\nlabels = [\"Poor (300-579)\", \"Fair (580-669)\", \"Good (670-739)\", \"Very Good (740-799)\", \"Excellent (800+)\"]\naxs = axs.flatten()\n\nfor ag, credit_score in enumerate(credit_score_ranges):\n    data = [\n        train[(train[\"Exited\"] == 0) & (train[\"CreditScore\"] >= credit_score[0]) & (train[\"CreditScore\"] < credit_score[1])][\"id\"].count(),\n        train[(train[\"Exited\"] == 1) & (train[\"CreditScore\"] >= credit_score[0]) & (train[\"CreditScore\"] < credit_score[1])][\"id\"].count()\n    ]\n\n    label = [\"Did Not Exit\", \"Exited\"]\n    _ = axs[ag].pie(\n        data, labels=label,\n        autopct=lambda x: \"{:,.0f} = {:.2f}%\".format(x * sum(data)/100, x),\n        explode=[0.05] * 2, \n        pctdistance=0.5, \n        colors=sns.color_palette(\"Set2\")[0:2],\n    )\n    _ = axs[ag].set_title(f\"{labels[ag]}\", fontsize=15)\n    \naxs[5].set_axis_off()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:36.487229Z","iopub.execute_input":"2024-01-11T11:56:36.488145Z","iopub.status.idle":"2024-01-11T11:56:37.365904Z","shell.execute_reply.started":"2024-01-11T11:56:36.488117Z","shell.execute_reply":"2024-01-11T11:56:37.363481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In general, gradient boosting classifiers are likely to already identify this type of data partitioning. The question remains whether this generalized rating is simply too coarse to be of much use. More specifically, if we narrow the boundaries between credit ratings, can we achieve a better class separation?","metadata":{}},{"cell_type":"code","source":"def bin_data(series, bin_defs):\n    bins = [0 for _ in range(len(bin_defs))]\n    total = 0\n    for x in series:\n        for index, (bin_min, bin_max) in enumerate(bin_defs):\n            if x >= bin_min and x < bin_max:\n                bins[index] += 1\n                total += 1\n                break\n    return [float(x / total) if total != 0 else 0 for x in bins]\n\nbin_defs = []\nlabels = []\nstep_size = 10\nfor x in range(400, 860, step_size):\n    bin_defs.append([x, x+step_size])\n    labels.append(\"{:.2f} - {:.2f}\".format(x, x+step_size))\n    \nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 10))\n\nsns.set_style('darkgrid')\nsns.set_palette('Set2')\n\ntdf = pd.DataFrame(\n    {\n        'Label': labels,\n        'Did Not Exit': bin_data(train[(train[\"Exited\"] == 0)][\"CreditScore\"], bin_defs),\n        'Exited': bin_data(train[(train[\"Exited\"] == 1)][\"CreditScore\"], bin_defs),\n    }\n)\n\n_ = tdf.set_index('Label').plot(kind='bar', stacked=True, color=[\"blue\", \"red\"], ax=ax)\n_ = ax.set_title(\"Credit Score vs Exited\", fontsize=15)\n_ = ax.set_ylabel(\"\")\n_ = ax.set_xlabel(\"Credit Score\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:37.368606Z","iopub.execute_input":"2024-01-11T11:56:37.369083Z","iopub.status.idle":"2024-01-11T11:56:40.317234Z","shell.execute_reply.started":"2024-01-11T11:56:37.369053Z","shell.execute_reply":"2024-01-11T11:56:40.315705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, in terms of classifying based on a credit score, we want to see columns that are very clearly defined as having the majority as being the positive case or negative case. Again, we're not seeing that type of separation. If we more even finger-grained:","metadata":{}},{"cell_type":"code","source":"bin_defs = []\nlabels = []\nstep_size = 5\nfor x in range(400, 860, step_size):\n    bin_defs.append([x, x+step_size])\n    labels.append(\"{:.2f} - {:.2f}\".format(x, x+step_size))\n    \nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 10))\n\nsns.set_style('darkgrid')\nsns.set_palette('Set2')\n\ntdf = pd.DataFrame(\n    {\n        'Label': labels,\n        'Did Not Exit': bin_data(train[(train[\"Exited\"] == 0)][\"CreditScore\"], bin_defs),\n        'Exited': bin_data(train[(train[\"Exited\"] == 1)][\"CreditScore\"], bin_defs),\n    }\n)\n\n_ = tdf.set_index('Label').plot(kind='bar', stacked=True, color=[\"blue\", \"red\"], ax=ax)\n_ = ax.set_title(\"Credit Score vs Exited\", fontsize=15)\n_ = ax.set_ylabel(\"\")\n_ = ax.set_xlabel(\"Credit Score\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:40.319363Z","iopub.execute_input":"2024-01-11T11:56:40.320696Z","iopub.status.idle":"2024-01-11T11:56:43.541540Z","shell.execute_reply.started":"2024-01-11T11:56:40.320657Z","shell.execute_reply":"2024-01-11T11:56:43.538805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There simply is not a good slicing that would provide us class separation. We can use some kernel density plots to confirm what we're seeing as well.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 8))\n\nsns.set_style('darkgrid')\n\n_ = sns.kdeplot(train[(train[\"Exited\"] == 1)][\"CreditScore\"], shade=True, color=\"r\", ax=ax, label=\"Exited\")\n_ = sns.kdeplot(train[(train[\"Exited\"] == 0)][\"CreditScore\"], shade=True, color=\"b\", ax=ax, label=\"No Exit\")\n_ = ax.set_title(\"Exited Findings by CreditScore\".format(feature), fontsize=15)\n_ = ax.set_ylabel(\"\")\n_ = ax.set_xlabel(\"\")\nhandles, labels = ax.get_legend_handles_labels()\n_ = ax.legend(handles=handles[0:2], labels=labels[0:2], title=\"\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:43.544155Z","iopub.execute_input":"2024-01-11T11:56:43.546007Z","iopub.status.idle":"2024-01-11T11:56:44.731798Z","shell.execute_reply.started":"2024-01-11T11:56:43.545940Z","shell.execute_reply":"2024-01-11T11:56:44.729912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, there are no places on the plot where we clearly see instances where just the positive class occurring.\n\n### Key Observations about Credit Score\n\n* Credit score as a first order feature, does not appear to provide adequate signal to determine if churn is occurring.","metadata":{}},{"cell_type":"markdown","source":"# 2.3 - Geography\n\nThe `Geography` feature appears to be categorical in nature, with only three different values. Let's look a their correlation to `Exited`. ","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\n\nsns.set_style('darkgrid')\nsns.set_palette('Set2')\n\ngeographies = [place for place in train['Geography'].unique()]\naxs = axs.flatten()\n\nfor ag, geography in enumerate(geographies):\n    data = [\n        train[(train[\"Exited\"] == 0) & (train[\"Geography\"] == geography)][\"id\"].count(),\n        train[(train[\"Exited\"] == 1) & (train[\"Geography\"] == geography)][\"id\"].count()\n    ]\n\n    label = [\"Did Not Exit\", \"Exited\"]\n    _ = axs[ag].pie(\n        data, labels=[\"Did Not Exit\", \"Exited\"],\n        autopct=lambda x: \"{:,.0f} = {:.2f}%\".format(x * sum(data)/100, x),\n        explode=[0.05] * 2, \n        pctdistance=0.5, \n        colors=sns.color_palette(\"Set2\")[0:2],\n    )\n    _ = axs[ag].set_title(f\"{geography}\", fontsize=15)\n    \naxs[3].set_axis_off()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:44.733490Z","iopub.execute_input":"2024-01-11T11:56:44.733868Z","iopub.status.idle":"2024-01-11T11:56:45.391241Z","shell.execute_reply.started":"2024-01-11T11:56:44.733841Z","shell.execute_reply":"2024-01-11T11:56:45.389511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A little bit of insight comes from the fact that `Germany` appears to have a higher number of patrons that exit a bank than others do. Alone this may not be enough of an insight to provide our classifier with any lift, but in combination with other risk factors, it may provide a second-order feature that provides lift.\n\nWhile our Spearman correlation didn't show strong connection to `Exited`, it did however have a strong correlation to `Balance`, which in turn does have a high connection to `Exited`. Let's take a look for a moment at `Balance` and `Geography`.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(15, 5))\n\n_ = sns.kdeplot(train[(train[\"Geography\"] == \"Germany\")][\"Balance\"], shade=True, color=\"r\", ax=axs, label=\"Germany\")\n_ = sns.kdeplot(train[(train[\"Geography\"] == \"France\")][\"Balance\"], shade=True, color=\"g\", ax=axs, label=\"France\")\n_ = sns.kdeplot(train[(train[\"Geography\"] == \"Spain\")][\"Balance\"], shade=True, color=\"b\", ax=axs, label=\"Spain\")\n_ = axs.set_title(\"Balance Densities by Geography\", fontsize=15)\n_ = axs.set_ylabel(\"Density\")\n_ = axs.set_xlabel(\"Balance\")\n_ = axs.legend()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:45.393797Z","iopub.execute_input":"2024-01-11T11:56:45.394205Z","iopub.status.idle":"2024-01-11T11:56:46.624785Z","shell.execute_reply.started":"2024-01-11T11:56:45.394170Z","shell.execute_reply":"2024-01-11T11:56:46.621696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, there is a very noticeable difference in terms of balance distributions between various geographies, with `Germany` being the one geography that has the bulk of it's balances sitting at roughly $125,000. Again, this doesn't help us with the `Exited` value, but it does show us there are differences between geographic locations.\n\n### Key Insights about Geography\n\n* The breakdown of `Exited` by geography didn't provide a clear indicator for our classifier.\n    * However, we may be able to use `Germany` as a risk factor calculation, since it is clear that people leaving occurs more in Germany than other geographic locations.","metadata":{}},{"cell_type":"markdown","source":"# 2.4 - Gender\n\nOur Spearman correlation showed us that there was a weak negative correlation between the `Gender` feature and `Exited`. Let's take a look at gender versus our target to see if there is anything interesting.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 15))\n\nsns.set_style('darkgrid')\nsns.set_palette('Set2')\n\ngenders = [place for place in train['Gender'].unique()]\naxs = axs.flatten()\n\nfor ag, gender in enumerate(genders):\n    data = [\n        train[(train[\"Exited\"] == 0) & (train[\"Gender\"] == gender)][\"id\"].count(),\n        train[(train[\"Exited\"] == 1) & (train[\"Gender\"] == gender)][\"id\"].count()\n    ]\n\n    label = [\"Did Not Exit\", \"Exited\"]\n    _ = axs[ag].pie(\n        data, labels=[\"Did Not Exit\", \"Exited\"],\n        autopct=lambda x: \"{:,.0f} = {:.2f}%\".format(x * sum(data)/100, x),\n        explode=[0.05] * 2, \n        pctdistance=0.5, \n        colors=sns.color_palette(\"Set2\")[0:2],\n    )\n    _ = axs[ag].set_title(f\"{gender}\", fontsize=15)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:46.630308Z","iopub.execute_input":"2024-01-11T11:56:46.630827Z","iopub.status.idle":"2024-01-11T11:56:46.981838Z","shell.execute_reply.started":"2024-01-11T11:56:46.630787Z","shell.execute_reply":"2024-01-11T11:56:46.979590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, patrons who are `Male` are much more likely to stay with the bank when compared with those that are `Female`. The question is whether there are even clearer indications of when `Male` patrons may leave. Let's take a look at `Gender` combined with `Geography`. Specifically, let's look at `Male` patrons, since they have a different likelihood of exiting.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\n\nsns.set_style('darkgrid')\nsns.set_palette('Set2')\n\ngeographies = [place for place in train['Geography'].unique()]\naxs = axs.flatten()\n\nfor ag, geography in enumerate(geographies):\n    data = [\n        train[(train[\"Exited\"] == 0) & (train[\"Geography\"] == geography) & (train[\"Gender\"] == \"Male\")][\"id\"].count(),\n        train[(train[\"Exited\"] == 1) & (train[\"Geography\"] == geography) & (train[\"Gender\"] == \"Male\")][\"id\"].count()\n    ]\n\n    label = [\"Did Not Exit\", \"Exited\"]\n    _ = axs[ag].pie(\n        data, labels=[\"Did Not Exit\", \"Exited\"],\n        autopct=lambda x: \"{:,.0f} = {:.2f}%\".format(x * sum(data)/100, x),\n        explode=[0.05] * 2, \n        pctdistance=0.5, \n        colors=sns.color_palette(\"Set2\")[0:2],\n    )\n    _ = axs[ag].set_title(f\"Male Exited Counts in {geography}\", fontsize=15)\n    \naxs[3].set_axis_off()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:46.984020Z","iopub.execute_input":"2024-01-11T11:56:46.985405Z","iopub.status.idle":"2024-01-11T11:56:47.731590Z","shell.execute_reply.started":"2024-01-11T11:56:46.985364Z","shell.execute_reply":"2024-01-11T11:56:47.730185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, we see lower numbers of males exiting from France and Spain. Again, this is not a strong enough indicator on its own, but could be combined into a _mitigating factors_ feature that counts the number of items that make it more _likely_ that a patron will stay with the bank. Again, models such as Gradient Boosting Decision Tree approaches are very likely to find this connection without any further input or processing. Let's take a look for a moment at balances.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(15, 5))\n\n_ = sns.kdeplot(train[(train[\"Gender\"] == \"Male\")][\"Balance\"], shade=True, color=\"r\", ax=axs, label=\"Male\")\n_ = sns.kdeplot(train[(train[\"Gender\"] == \"Female\")][\"Balance\"], shade=True, color=\"b\", ax=axs, label=\"Female\")\n_ = axs.set_title(\"Balance Densities by Gender\", fontsize=15)\n_ = axs.set_ylabel(\"Density\")\n_ = axs.set_xlabel(\"Balance\")\n_ = axs.legend()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:47.733739Z","iopub.execute_input":"2024-01-11T11:56:47.734147Z","iopub.status.idle":"2024-01-11T11:56:48.780424Z","shell.execute_reply.started":"2024-01-11T11:56:47.734111Z","shell.execute_reply":"2024-01-11T11:56:48.779386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see nearly identical balance densities. If we break this down further by geographical location:","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(15, 15))\n\n_ = sns.kdeplot(train[(train[\"Gender\"] == \"Male\") & (train[\"Geography\"] == \"Germany\")][\"Balance\"], shade=True, color=\"r\", ax=axs[0], label=\"Male\")\n_ = sns.kdeplot(train[(train[\"Gender\"] == \"Female\") & (train[\"Geography\"] == \"Germany\")][\"Balance\"], shade=True, color=\"b\", ax=axs[0], label=\"Female\")\n_ = axs[0].set_title(\"Balance Densities by Gender in Germany\", fontsize=15)\n_ = axs[0].set_ylabel(\"Density\")\n_ = axs[0].set_xlabel(\"\")\n_ = axs[0].legend()\n\n_ = sns.kdeplot(train[(train[\"Gender\"] == \"Male\") & (train[\"Geography\"] == \"France\")][\"Balance\"], shade=True, color=\"r\", ax=axs[1], label=\"Male\")\n_ = sns.kdeplot(train[(train[\"Gender\"] == \"Female\") & (train[\"Geography\"] == \"France\")][\"Balance\"], shade=True, color=\"b\", ax=axs[1], label=\"Female\")\n_ = axs[1].set_title(\"Balance Densities by Gender in France\", fontsize=15)\n_ = axs[1].set_ylabel(\"Density\")\n_ = axs[1].set_xlabel(\"\")\n_ = axs[1].legend()\n\n_ = sns.kdeplot(train[(train[\"Gender\"] == \"Male\") & (train[\"Geography\"] == \"Spain\")][\"Balance\"], shade=True, color=\"r\", ax=axs[2], label=\"Male\")\n_ = sns.kdeplot(train[(train[\"Gender\"] == \"Female\") & (train[\"Geography\"] == \"Spain\")][\"Balance\"], shade=True, color=\"b\", ax=axs[2], label=\"Female\")\n_ = axs[2].set_title(\"Balance Densities by Gender in Spain\", fontsize=15)\n_ = axs[2].set_ylabel(\"Density\")\n_ = axs[2].set_xlabel(\"\")\n_ = axs[2].legend()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:48.782143Z","iopub.execute_input":"2024-01-11T11:56:48.782800Z","iopub.status.idle":"2024-01-11T11:56:51.171153Z","shell.execute_reply.started":"2024-01-11T11:56:48.782752Z","shell.execute_reply":"2024-01-11T11:56:51.168829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, balance densities are very similar between genders regardless of geography.","metadata":{}},{"cell_type":"markdown","source":"### Key Observations about Gender\n\n* Males are far less likely to exit the bank.\n    * This holds true in both `Spain` and `France`.\n    * Males in `Germany` are much more likely to exit when compared to other geographies.\n* Males and females have very similar balances.","metadata":{}},{"cell_type":"markdown","source":"# 2.5 - Age\n\nFirst of all, let's take a look at just the `Age` feature alone to see whether we have class separation.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 8))\n\nsns.set_style('darkgrid')\n\n_ = sns.kdeplot(train[(train[\"Exited\"] == 1)][\"Age\"], shade=True, color=\"r\", ax=ax, label=\"Exited\")\n_ = sns.kdeplot(train[(train[\"Exited\"] == 0)][\"Age\"], shade=True, color=\"b\", ax=ax, label=\"No Exit\")\n_ = ax.set_title(\"Exited Findings by Age\".format(feature), fontsize=15)\n_ = ax.set_ylabel(\"\")\n_ = ax.set_xlabel(\"\")\nhandles, labels = ax.get_legend_handles_labels()\n_ = ax.legend(handles=handles[0:2], labels=labels[0:2], title=\"\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:51.174577Z","iopub.execute_input":"2024-01-11T11:56:51.175197Z","iopub.status.idle":"2024-01-11T11:56:52.399665Z","shell.execute_reply.started":"2024-01-11T11:56:51.175157Z","shell.execute_reply":"2024-01-11T11:56:52.398594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have two very distinct age densities associated with `Exited`. As we can see, those aged 25-35 are likely to remain with the bank, while those aged 40 - 65 are more likely to exit. This finding is consistent with our Spearman correlation, which suggested that higher ages were more strongly correlated with the target. As a first-order feature, this is good news, since it suggests that one of the best indicators of churn is age. Much like our `Geography` column, we can also use this as an absolute value for a risk factor. If age is greater than 40, an additional risk factor exists.\n\nThe question is whether we can gain additional lift by enhancing the feature with other continuous columns. A good candidate is the number of products that the user has, since our Spearman correlation said that the number of products was negatively correlated with the target variable (the more products a patron has, the less likely they are to leave). If we exaggerate age by number of products, we may be able to gain class separation.","metadata":{}},{"cell_type":"code","source":"train[\"Age_NumOfProducts\"] =  train[\"Age\"] ** train[\"NumOfProducts\"]\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 8))\n\nsns.set_style('darkgrid')\n\n_ = sns.kdeplot(train[(train[\"Exited\"] == 1)][\"Age_NumOfProducts\"], shade=True, color=\"r\", ax=ax, label=\"Exited\")\n_ = sns.kdeplot(train[(train[\"Exited\"] == 0)][\"Age_NumOfProducts\"], shade=True, color=\"b\", ax=ax, label=\"No Exit\")\n_ = ax.set_title(\"Exited Findings by Age ** NumOfProducts\".format(feature), fontsize=15)\n_ = ax.set_ylabel(\"\")\n_ = ax.set_xlabel(\"\")\n_ = ax.set_xlim(0, 0.045e7)\nhandles, labels = ax.get_legend_handles_labels()\n_ = ax.legend(handles=handles[0:2], labels=labels[0:2], title=\"\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:52.406094Z","iopub.execute_input":"2024-01-11T11:56:52.406715Z","iopub.status.idle":"2024-01-11T11:56:53.959952Z","shell.execute_reply.started":"2024-01-11T11:56:52.406678Z","shell.execute_reply":"2024-01-11T11:56:53.959176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the plot above, we have zoomed in substantially. We can see that we do have quite a large class separation between those that exited and those that didn't. What the plot isn't showing is the long tail for the `No Exit` grouping. This particular feature combination may provide additional lift to our classifier.\n\n### Key Observations about Age\n\n* Age is a strong indicator for our target.\n* We may be able to create an additional risk factor based on age.\n* Age combined with number of products may provide lift as an additional feature.","metadata":{}},{"cell_type":"markdown","source":"# 2.6 - Tenure\n\nTenure refers to how long a patron has been with the institution. Let's take a look at raw tenure numbers and see how they pan out.","metadata":{}},{"cell_type":"code","source":"bin_defs = []\nlabels = []\nstep_size = 1\nfor x in range(0, 10, step_size):\n    bin_defs.append([x, x+step_size])\n    labels.append(f\"{x:d}\")\n    \nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 5))\n\nsns.set_style('darkgrid')\nsns.set_palette('Set2')\n\ntdf = pd.DataFrame(\n    {\n        'Label': labels,\n        'Did Not Exit': bin_data(train[(train[\"Exited\"] == 0)][\"Tenure\"], bin_defs),\n        'Exited': bin_data(train[(train[\"Exited\"] == 1)][\"Tenure\"], bin_defs),\n    }\n)\n\n_ = tdf.set_index('Label').plot(kind='bar', stacked=True, color=[\"blue\", \"red\"], ax=ax)\n_ = ax.set_title(\"Tenure vs Exited\", fontsize=15)\n_ = ax.set_ylabel(\"\")\n_ = ax.set_xlabel(\"Tenure (Years)\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:53.961192Z","iopub.execute_input":"2024-01-11T11:56:53.962085Z","iopub.status.idle":"2024-01-11T11:56:54.635482Z","shell.execute_reply.started":"2024-01-11T11:56:53.962055Z","shell.execute_reply":"2024-01-11T11:56:54.633924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, there isn't really a difference in raw tenure years. There are however, a number of ways to look at tenure statistics. First, let's see if there is a difference in `Gender`.","metadata":{}},{"cell_type":"code","source":"bin_defs = []\nlabels = []\nstep_size = 1\nfor x in range(0, 10, step_size):\n    bin_defs.append([x, x+step_size])\n    labels.append(f\"{x:d}\")\n    \nfig, axs = plt.subplots(nrows=2, ncols=1, figsize=(15, 10))\n\nsns.set_style('darkgrid')\nsns.set_palette('Set2')\n\ntdf = pd.DataFrame(\n    {\n        'Label': labels,\n        'Did Not Exit': bin_data(train[(train[\"Exited\"] == 0) & (train[\"Gender\"] == \"Male\")][\"Tenure\"], bin_defs),\n        'Exited': bin_data(train[(train[\"Exited\"] == 1) & (train[\"Gender\"] == \"Male\")][\"Tenure\"], bin_defs),\n    }\n)\n\n_ = tdf.set_index('Label').plot(kind='bar', stacked=True, color=[\"blue\", \"red\"], ax=axs[0])\n_ = axs[0].set_title(\"Tenure vs Exited For Male Population\", fontsize=15)\n_ = axs[0].set_ylabel(\"\")\n_ = axs[0].set_xlabel(\"\")\n\ntdf = pd.DataFrame(\n    {\n        'Label': labels,\n        'Did Not Exit': bin_data(train[(train[\"Exited\"] == 0) & (train[\"Gender\"] == \"Female\")][\"Tenure\"], bin_defs),\n        'Exited': bin_data(train[(train[\"Exited\"] == 1) & (train[\"Gender\"] == \"Female\")][\"Tenure\"], bin_defs),\n    }\n)\n\n_ = tdf.set_index('Label').plot(kind='bar', stacked=True, color=[\"blue\", \"red\"], ax=axs[1])\n_ = axs[1].set_title(\"Tenure vs Exited For Female Population\", fontsize=15)\n_ = axs[1].set_ylabel(\"\")\n_ = axs[1].set_xlabel(\"\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:54.637604Z","iopub.execute_input":"2024-01-11T11:56:54.637965Z","iopub.status.idle":"2024-01-11T11:56:55.730048Z","shell.execute_reply.started":"2024-01-11T11:56:54.637930Z","shell.execute_reply":"2024-01-11T11:56:55.728558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No magic bullets. Let's look at tenure based on geography.","metadata":{}},{"cell_type":"code","source":"bin_defs = []\nlabels = []\nstep_size = 1\nfor x in range(0, 10, step_size):\n    bin_defs.append([x, x+step_size])\n    labels.append(f\"{x:d}\")\n    \nfig, axs = plt.subplots(nrows=3, ncols=1, figsize=(15, 15))\n\nsns.set_style('darkgrid')\nsns.set_palette('Set2')\n\ntdf = pd.DataFrame(\n    {\n        'Label': labels,\n        'Did Not Exit': bin_data(train[(train[\"Exited\"] == 0) & (train[\"Geography\"] == \"Germany\")][\"Tenure\"], bin_defs),\n        'Exited': bin_data(train[(train[\"Exited\"] == 1) & (train[\"Geography\"] == \"Germany\")][\"Tenure\"], bin_defs),\n    }\n)\n\n_ = tdf.set_index('Label').plot(kind='bar', stacked=True, color=[\"blue\", \"red\"], ax=axs[0])\n_ = axs[0].set_title(\"Tenure vs Exited in Germany\", fontsize=15)\n_ = axs[0].set_ylabel(\"\")\n_ = axs[0].set_xlabel(\"\")\n\ntdf = pd.DataFrame(\n    {\n        'Label': labels,\n        'Did Not Exit': bin_data(train[(train[\"Exited\"] == 0) & (train[\"Geography\"] == \"Spain\")][\"Tenure\"], bin_defs),\n        'Exited': bin_data(train[(train[\"Exited\"] == 1) & (train[\"Geography\"] == \"Spain\")][\"Tenure\"], bin_defs),\n    }\n)\n\n_ = tdf.set_index('Label').plot(kind='bar', stacked=True, color=[\"blue\", \"red\"], ax=axs[1])\n_ = axs[1].set_title(\"Tenure vs Exited in Spain\", fontsize=15)\n_ = axs[1].set_ylabel(\"\")\n_ = axs[1].set_xlabel(\"\")\n\ntdf = pd.DataFrame(\n    {\n        'Label': labels,\n        'Did Not Exit': bin_data(train[(train[\"Exited\"] == 0) & (train[\"Geography\"] == \"France\")][\"Tenure\"], bin_defs),\n        'Exited': bin_data(train[(train[\"Exited\"] == 1) & (train[\"Geography\"] == \"France\")][\"Tenure\"], bin_defs),\n    }\n)\n\n_ = tdf.set_index('Label').plot(kind='bar', stacked=True, color=[\"blue\", \"red\"], ax=axs[2])\n_ = axs[2].set_title(\"Tenure vs Exited in France\", fontsize=15)\n_ = axs[2].set_ylabel(\"\")\n_ = axs[2].set_xlabel(\"\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:55.731843Z","iopub.execute_input":"2024-01-11T11:56:55.732228Z","iopub.status.idle":"2024-01-11T11:56:57.307076Z","shell.execute_reply.started":"2024-01-11T11:56:55.732181Z","shell.execute_reply":"2024-01-11T11:56:57.305377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, no particular tenure year based on geography appears to stand out. Let's see if age plays a factor with tenure.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 10))\n\nsns.set_style('darkgrid')\n\n_ = sns.violinplot(data=train, x=\"Tenure\", y=\"Age\", hue=\"Exited\", split=True, inner=\"quart\")\n_ = ax.set_title(\"Exited Age vs Tenure\", fontsize=15)\n_ = ax.set_ylabel(\"Age\")\n_ = ax.set_xlabel(\"Tenure\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:57.309762Z","iopub.execute_input":"2024-01-11T11:56:57.310232Z","iopub.status.idle":"2024-01-11T11:56:58.762096Z","shell.execute_reply.started":"2024-01-11T11:56:57.310197Z","shell.execute_reply":"2024-01-11T11:56:58.760263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, no significant difference between tenure groups and the relative ages when people exit. ","metadata":{}},{"cell_type":"markdown","source":"# 2.8 - Balance\n\nTo start with, let's cut right to a density plot and see if the `Balance` feature shows any clear separation between those who exited, and those who didn't. Given the high correlation between `Balance` and `Exited`, we expect to see some separation.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 8))\n\nsns.set_style('darkgrid')\n\n_ = sns.kdeplot(train[(train[\"Exited\"] == 1)][\"Balance\"], shade=True, color=\"r\", ax=ax, label=\"Exited\")\n_ = sns.kdeplot(train[(train[\"Exited\"] == 0)][\"Balance\"], shade=True, color=\"b\", ax=ax, label=\"Did Not Exit\")\n_ = ax.set_title(\"Density of Balances\".format(feature), fontsize=15)\n_ = ax.set_ylabel(\"\")\n_ = ax.set_xlabel(\"\")\n_ = ax.set_xlim(0, 250000)\nhandles, labels = ax.get_legend_handles_labels()\n_ = ax.legend(handles=handles[0:2], labels=labels[0:2], title=\"\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:58.764397Z","iopub.execute_input":"2024-01-11T11:56:58.764921Z","iopub.status.idle":"2024-01-11T11:56:59.871917Z","shell.execute_reply.started":"2024-01-11T11:56:58.764889Z","shell.execute_reply":"2024-01-11T11:56:59.870613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, it appears that having a $0 balance would be a mitigating factor in regards to whether a patron exits. This does raise an interesting point: why be a patron when you have no balance? Perhaps balances vs number of products will tell us.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 10))\n\nsns.set_style('darkgrid')\n\n_ = sns.violinplot(data=train, x=\"NumOfProducts\", y=\"Balance\", hue=\"Exited\", split=True, inner=\"quart\")\n_ = ax.set_title(\"Exited Balance vs NumOfProducts\", fontsize=15)\n_ = ax.set_ylabel(\"Balance\")\n_ = ax.set_xlabel(\"NumOfProducts\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:56:59.873907Z","iopub.execute_input":"2024-01-11T11:56:59.874247Z","iopub.status.idle":"2024-01-11T11:57:00.777226Z","shell.execute_reply.started":"2024-01-11T11:56:59.874220Z","shell.execute_reply":"2024-01-11T11:57:00.775508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is starting to look more interesting. While having 1 or 4 products doesn't help us separate out those that exit from those that don't, the interesting cases occur when there are 2 or 3 products. To summarize:\n\n* 2 products and $0 balance means a user is far less likely to exit.\n* 3 products and a $0 balance means a user is far less likely to exit.\n* 3 products and a balance means the user is far more likely to exit.\n\nWe can use this information to our advantage as more refined mitigating or risk factors.","metadata":{}},{"cell_type":"markdown","source":"# 2.9 - NumOfProducts\n\n_ In progress..._","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 8))\n\nsns.set_style('darkgrid')\n\n_ = sns.kdeplot(train[(train[\"Exited\"] == 1)][\"NumOfProducts\"], shade=True, color=\"r\", ax=ax, label=\"Exited\")\n_ = sns.kdeplot(train[(train[\"Exited\"] == 0)][\"NumOfProducts\"], shade=True, color=\"b\", ax=ax, label=\"No Exit\")\n_ = ax.set_title(\"Exited Findings by NumOfProducts\".format(feature), fontsize=15)\n_ = ax.set_ylabel(\"\")\n_ = ax.set_xlabel(\"\")\nhandles, labels = ax.get_legend_handles_labels()\n_ = ax.legend(handles=handles[0:2], labels=labels[0:2], title=\"\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:57:00.780354Z","iopub.execute_input":"2024-01-11T11:57:00.780843Z","iopub.status.idle":"2024-01-11T11:57:02.060560Z","shell.execute_reply.started":"2024-01-11T11:57:00.780806Z","shell.execute_reply":"2024-01-11T11:57:02.058198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 10))\n\nsns.set_style('darkgrid')\n\n_ = sns.violinplot(data=train, x=\"NumOfProducts\", y=\"Age\", hue=\"Exited\", split=True, inner=\"quart\")\n_ = ax.set_title(\"Exited Age vs NumOfProducts\", fontsize=15)\n_ = ax.set_ylabel(\"Age\")\n_ = ax.set_xlabel(\"NumOfProducts\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:57:02.062986Z","iopub.execute_input":"2024-01-11T11:57:02.063474Z","iopub.status.idle":"2024-01-11T11:57:03.056947Z","shell.execute_reply.started":"2024-01-11T11:57:02.063445Z","shell.execute_reply":"2024-01-11T11:57:03.054860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.10 - HasCrCard\n\n_In progress..._","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 15))\n\nsns.set_style('darkgrid')\nsns.set_palette('Set2')\n\nhas_crcard_types = [place for place in train['HasCrCard'].unique()]\naxs = axs.flatten()\n\nfor ag, has_crcard in enumerate(has_crcard_types):\n    data = [\n        train[(train[\"Exited\"] == 0) & (train[\"HasCrCard\"] == has_crcard)][\"id\"].count(),\n        train[(train[\"Exited\"] == 1) & (train[\"HasCrCard\"] == has_crcard)][\"id\"].count()\n    ]\n\n    label = [\"Did Not Exit\", \"Exited\"]\n    _ = axs[ag].pie(\n        data, labels=[\"Did Not Exit\", \"Exited\"],\n        autopct=lambda x: \"{:,.0f} = {:.2f}%\".format(x * sum(data)/100, x),\n        explode=[0.05] * 2, \n        pctdistance=0.5, \n        colors=sns.color_palette(\"Set2\")[0:2],\n    )\n    _ = axs[ag].set_title(f\"{'Has Credit Card' if has_crcard == 1 else 'No Credit Card'}\", fontsize=15)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:57:03.058784Z","iopub.execute_input":"2024-01-11T11:57:03.059105Z","iopub.status.idle":"2024-01-11T11:57:03.399406Z","shell.execute_reply.started":"2024-01-11T11:57:03.059081Z","shell.execute_reply":"2024-01-11T11:57:03.397349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 10))\n\nsns.set_style('darkgrid')\n\n_ = sns.violinplot(data=train, x=\"NumOfProducts\", y=\"HasCrCard\", hue=\"Exited\", split=True, inner=\"quart\")\n_ = ax.set_title(\"Exited HasCrCard vs NumOfProducts\", fontsize=15)\n_ = ax.set_ylabel(\"HasCrCard\")\n_ = ax.set_xlabel(\"NumOfProducts\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:57:03.401598Z","iopub.execute_input":"2024-01-11T11:57:03.402018Z","iopub.status.idle":"2024-01-11T11:57:04.335668Z","shell.execute_reply.started":"2024-01-11T11:57:03.401984Z","shell.execute_reply":"2024-01-11T11:57:04.332216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.11 - IsActiveMember\n\n_In progress..._","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 15))\n\nsns.set_style('darkgrid')\nsns.set_palette('Set2')\n\nis_active_status_types = [place for place in train['IsActiveMember'].unique()]\naxs = axs.flatten()\n\nfor ag, is_active in enumerate(is_active_status_types):\n    data = [\n        train[(train[\"Exited\"] == 0) & (train[\"IsActiveMember\"] == is_active)][\"id\"].count(),\n        train[(train[\"Exited\"] == 1) & (train[\"IsActiveMember\"] == is_active)][\"id\"].count()\n    ]\n\n    label = [\"Did Not Exit\", \"Exited\"]\n    _ = axs[ag].pie(\n        data, labels=[\"Did Not Exit\", \"Exited\"],\n        autopct=lambda x: \"{:,.0f} = {:.2f}%\".format(x * sum(data)/100, x),\n        explode=[0.05] * 2, \n        pctdistance=0.5, \n        colors=sns.color_palette(\"Set2\")[0:2],\n    )\n    _ = axs[ag].set_title(f\"{'Active' if is_active == 1 else 'Not Active'}\", fontsize=15)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:57:04.337092Z","iopub.execute_input":"2024-01-11T11:57:04.337487Z","iopub.status.idle":"2024-01-11T11:57:04.635110Z","shell.execute_reply.started":"2024-01-11T11:57:04.337459Z","shell.execute_reply":"2024-01-11T11:57:04.634187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.12 - EstimatedSalary\n\n_In progress..._","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 8))\n\nsns.set_style('darkgrid')\n\n_ = sns.kdeplot(train[(train[\"Exited\"] == 1)][\"EstimatedSalary\"], shade=True, color=\"r\", ax=ax, label=\"Exited\")\n_ = sns.kdeplot(train[(train[\"Exited\"] == 0)][\"EstimatedSalary\"], shade=True, color=\"b\", ax=ax, label=\"Did Not Exit\")\n_ = ax.set_title(\"Density of EstimatedSalary\".format(feature), fontsize=15)\n_ = ax.set_ylabel(\"\")\n_ = ax.set_xlabel(\"\")\n_ = ax.set_xlim(0, 250000)\nhandles, labels = ax.get_legend_handles_labels()\n_ = ax.legend(handles=handles[0:2], labels=labels[0:2], title=\"\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:57:04.636330Z","iopub.execute_input":"2024-01-11T11:57:04.637062Z","iopub.status.idle":"2024-01-11T11:57:05.687494Z","shell.execute_reply.started":"2024-01-11T11:57:04.637029Z","shell.execute_reply":"2024-01-11T11:57:05.686016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 10))\n\nsns.set_style('darkgrid')\n\n_ = sns.violinplot(data=train, x=\"NumOfProducts\", y=\"EstimatedSalary\", hue=\"Exited\", split=True, inner=\"quart\")\n_ = ax.set_title(\"Exited EstimatedSalary vs NumOfProducts\", fontsize=15)\n_ = ax.set_ylabel(\"EstimatedSalary\")\n_ = ax.set_xlabel(\"NumOfProducts\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:57:05.688958Z","iopub.execute_input":"2024-01-11T11:57:05.689459Z","iopub.status.idle":"2024-01-11T11:57:06.592345Z","shell.execute_reply.started":"2024-01-11T11:57:05.689262Z","shell.execute_reply":"2024-01-11T11:57:06.590621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 10))\n\nsns.set_style('darkgrid')\n\n_ = sns.violinplot(data=train, x=\"HasCrCard\", y=\"EstimatedSalary\", hue=\"Exited\", split=True, inner=\"quart\")\n_ = ax.set_title(\"Exited EstimatedSalary vs HasCrCard\", fontsize=15)\n_ = ax.set_ylabel(\"EstimatedSalary\")\n_ = ax.set_xlabel(\"HasCrCard\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:57:06.594258Z","iopub.execute_input":"2024-01-11T11:57:06.594648Z","iopub.status.idle":"2024-01-11T11:57:07.407459Z","shell.execute_reply.started":"2024-01-11T11:57:06.594619Z","shell.execute_reply":"2024-01-11T11:57:07.405900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"Future_Worth\"] = train[\"EstimatedSalary\"] + train[\"Balance\"]\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 8))\n\nsns.set_style('darkgrid')\n\n_ = sns.kdeplot(train[(train[\"Exited\"] == 1)][\"Future_Worth\"], shade=True, color=\"r\", ax=ax, label=\"Exited\")\n_ = sns.kdeplot(train[(train[\"Exited\"] == 0)][\"Future_Worth\"], shade=True, color=\"b\", ax=ax, label=\"Did Not Exit\")\n_ = ax.set_title(\"Density of Future_Worth\".format(feature), fontsize=15)\n_ = ax.set_ylabel(\"\")\n_ = ax.set_xlabel(\"\")\nhandles, labels = ax.get_legend_handles_labels()\n_ = ax.legend(handles=handles[0:2], labels=labels[0:2], title=\"\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:57:07.408895Z","iopub.execute_input":"2024-01-11T11:57:07.409591Z","iopub.status.idle":"2024-01-11T11:57:08.503085Z","shell.execute_reply.started":"2024-01-11T11:57:07.409479Z","shell.execute_reply":"2024-01-11T11:57:08.500597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 - Model Comparison\n\nLet's take a look at a few basic models, and see how well they perform with different types of features.\n\n# 3.1 - Vanilla LightGBM\n\nBased on the mix of categorical data, LightGBM is a good model to start with, since we can easily tell it what columns are categorical. We'll label encode the categorical features `Surname`, `Geography` and `Gender`.","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom lightgbm import early_stopping, log_evaluation\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfeatures = [\n    'Surname', 'CreditScore', 'Geography', 'Gender',\n    'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary'\n]\n\ntrain_copy = train.copy()\n\nfor feature in [\"Surname\", \"Geography\", \"Gender\"]:\n    le = LabelEncoder()\n    train_copy[feature] = le.fit_transform(train_copy[feature])\n\nn_folds = 5\nskf = StratifiedKFold(n_splits=n_folds, random_state=2023, shuffle=True)\ntrain_oof_preds = np.zeros((train.shape[0],))\nscores = []\n\nfor fold, (train_index, test_index) in enumerate(skf.split(train_copy, train_copy[\"Exited\"])):\n    print(f\"-------> Fold {fold+1} <--------\")\n    x_train, x_valid = pd.DataFrame(train_copy.iloc[train_index]), pd.DataFrame(train_copy.iloc[test_index])\n    y_train, y_valid = train_copy[\"Exited\"].iloc[train_index], train_copy[\"Exited\"].iloc[test_index]\n    \n    x_train_features = pd.DataFrame(x_train[features])\n    x_valid_features = pd.DataFrame(x_valid[features])\n\n    model = LGBMClassifier(\n        random_state=2023,\n        objective=\"binary\",\n        metric=\"auc\",\n        n_jobs=-1,\n        n_estimators=5000,\n        verbose=-1,    \n    )\n    model.fit(\n        x_train_features[features], \n        y_train,\n        eval_set=[(x_valid_features[features], y_valid)],\n        callbacks=[\n            early_stopping(50, verbose=False),\n            log_evaluation(5000),\n        ]\n    )\n    oof_preds = model.predict_proba(x_valid_features[features])[:,1]\n    train_oof_preds[test_index] = oof_preds\n    score = roc_auc_score(y_valid, oof_preds)\n    scores.append(score)\n    print(f\": AUC ROC = {score:.5f}\")\n    \nauc_baseline = roc_auc_score(train[\"Exited\"], train_oof_preds)\nprint(\"--> Overall results for out of fold predictions\")\nprint(f\": AUC ROC = {auc_baseline:.5f}\")\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 3))\n\ndata = pd.DataFrame({\"Fold\": [x + 1 for x in range(n_folds)], \"AUC ROC\": scores})\n_ = sns.lineplot(x=\"Fold\", y=\"AUC ROC\", data=data, ax=ax)\n_ = ax.set_title(\"AUC ROC per Fold\", fontsize=15)\n_ = ax.set_ylabel(\"AUC ROC\")\n_ = ax.set_xlabel(\"Fold #\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:57:08.506217Z","iopub.execute_input":"2024-01-11T11:57:08.507172Z","iopub.status.idle":"2024-01-11T11:57:39.713701Z","shell.execute_reply.started":"2024-01-11T11:57:08.507101Z","shell.execute_reply":"2024-01-11T11:57:39.712848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.calibration import CalibrationDisplay\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n\nCalibrationDisplay.from_predictions(train_copy[\"Exited\"], train_oof_preds, n_bins=30, strategy='quantile', ax=ax)\n_ = ax.set_title(\"Prediction Calibration\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:57:39.714886Z","iopub.execute_input":"2024-01-11T11:57:39.716188Z","iopub.status.idle":"2024-01-11T11:57:40.156543Z","shell.execute_reply.started":"2024-01-11T11:57:39.716120Z","shell.execute_reply":"2024-01-11T11:57:40.154538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With very little tuning, the classifier does very well, scoring an AUC ROC of 0.89121. The calibration of our predictions looks good too, with predictions fairly spread out along the probability spectrum, although there is some clumping toward the negative class. ","metadata":{}},{"cell_type":"markdown","source":"# 3.2 - LightGBM Categorical\n\nLightGBM has built-in support for dealing with categorical features, but has to be told which columns are to be interpreted as categorical. Features must be integer encoded. There are smoothing options to deal with categorical features with high cardinality, but we will ignore those for now and use default settings.","metadata":{}},{"cell_type":"code","source":"features = [\n    'Surname', 'Geography', 'Gender', 'CreditScore', \n    'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary'\n]\n\ntrain_copy = train.copy()\n\nfor feature in [\"Surname\", \"Geography\", \"Gender\"]:\n    le = LabelEncoder()\n    train_copy[feature] = le.fit_transform(train_copy[feature])\n\nn_folds = 5\nskf = StratifiedKFold(n_splits=n_folds, random_state=2023, shuffle=True)\ntrain_oof_preds = np.zeros((train.shape[0],))\nscores = []\n\nfor fold, (train_index, test_index) in enumerate(skf.split(train_copy, train_copy[\"Exited\"])):\n    print(f\"-------> Fold {fold+1} <--------\")\n    x_train, x_valid = pd.DataFrame(train_copy.iloc[train_index]), pd.DataFrame(train_copy.iloc[test_index])\n    y_train, y_valid = train_copy[\"Exited\"].iloc[train_index], train_copy[\"Exited\"].iloc[test_index]\n    \n    x_train_features = pd.DataFrame(x_train[features])\n    x_valid_features = pd.DataFrame(x_valid[features])\n\n    model = LGBMClassifier(\n        random_state=2023,\n        objective=\"binary\",\n        metric=\"auc\",\n        n_jobs=-1,\n        n_estimators=5000,\n        verbose=-1,    \n        categorical_features = [0, 1, 2],\n    )\n    model.fit(\n        x_train_features[features], \n        y_train,\n        eval_set=[(x_valid_features[features], y_valid)],\n        callbacks=[\n            early_stopping(50, verbose=False),\n            log_evaluation(5000),\n        ]\n    )\n    oof_preds = model.predict_proba(x_valid_features[features])[:,1]\n    train_oof_preds[test_index] = oof_preds\n    score = roc_auc_score(y_valid, oof_preds)\n    scores.append(score)\n    print(f\": AUC ROC = {score:.5f}\")\n    \nauc_category = roc_auc_score(train[\"Exited\"], train_oof_preds)\nprint(\"--> Overall results for out of fold predictions\")\nprint(f\": AUC ROC = {auc_category:.5f}\")\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 3))\n\ndata = pd.DataFrame({\"Fold\": [x + 1 for x in range(n_folds)], \"AUC ROC\": scores})\n_ = sns.lineplot(x=\"Fold\", y=\"AUC ROC\", data=data, ax=ax)\n_ = ax.set_title(\"AUC ROC per Fold\", fontsize=15)\n_ = ax.set_ylabel(\"AUC ROC\")\n_ = ax.set_xlabel(\"Fold #\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:57:40.157981Z","iopub.execute_input":"2024-01-11T11:57:40.158337Z","iopub.status.idle":"2024-01-11T11:57:59.274059Z","shell.execute_reply.started":"2024-01-11T11:57:40.158278Z","shell.execute_reply":"2024-01-11T11:57:59.272754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n\nCalibrationDisplay.from_predictions(train_copy[\"Exited\"], train_oof_preds, n_bins=30, strategy='quantile', ax=ax)\n_ = ax.set_title(\"Prediction Calibration\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:57:59.275444Z","iopub.execute_input":"2024-01-11T11:57:59.275808Z","iopub.status.idle":"2024-01-11T11:57:59.664350Z","shell.execute_reply.started":"2024-01-11T11:57:59.275766Z","shell.execute_reply":"2024-01-11T11:57:59.662825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, we achieve a slightly less performant result, probably due to the high cardinality of the `Surname` category.","metadata":{}},{"cell_type":"markdown","source":"# 3.3 - Feature: First Letter Surname Group\n\nAs discussed at the start of our EDA, using just the first letter of the surname may provide additional information to a classifier. Let's try that as a feature.","metadata":{}},{"cell_type":"code","source":"features = [\n    'Surname', 'CreditScore', 'Geography', 'Gender',\n    'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary'\n]\n\ntrain_copy = train.copy()\ntrain_copy[\"Surname_First_Letter\"] = train_copy[\"Surname\"].apply(lambda x: x[0])\nfeatures.append(\"Surname_First_Letter\")\n\nfor feature in [\"Surname\", \"Geography\", \"Gender\", \"Surname_First_Letter\"]:\n    le = LabelEncoder()\n    train_copy[feature] = le.fit_transform(train_copy[feature])\n\nn_folds = 5\nskf = StratifiedKFold(n_splits=n_folds, random_state=2023, shuffle=True)\ntrain_oof_preds = np.zeros((train.shape[0],))\nscores = []\n\nfor fold, (train_index, test_index) in enumerate(skf.split(train_copy, train_copy[\"Exited\"])):\n    print(f\"-------> Fold {fold+1} <--------\")\n    x_train, x_valid = pd.DataFrame(train_copy.iloc[train_index]), pd.DataFrame(train_copy.iloc[test_index])\n    y_train, y_valid = train_copy[\"Exited\"].iloc[train_index], train_copy[\"Exited\"].iloc[test_index]\n    \n    x_train_features = pd.DataFrame(x_train[features])\n    x_valid_features = pd.DataFrame(x_valid[features])\n\n    model = LGBMClassifier(\n        random_state=2023,\n        objective=\"binary\",\n        metric=\"auc\",\n        n_jobs=-1,\n        n_estimators=5000,\n        verbose=-1,    \n    )\n    model.fit(\n        x_train_features[features], \n        y_train,\n        eval_set=[(x_valid_features[features], y_valid)],\n        callbacks=[\n            early_stopping(50, verbose=False),\n            log_evaluation(5000),\n        ]\n    )\n    oof_preds = model.predict_proba(x_valid_features[features])[:,1]\n    train_oof_preds[test_index] = oof_preds\n    score = roc_auc_score(y_valid, oof_preds)\n    scores.append(score)\n    print(f\": AUC ROC = {score:.5f}\")\n    \nauc_first_letter_surname = roc_auc_score(train[\"Exited\"], train_oof_preds)\nprint(\"--> Overall results for out of fold predictions\")\nprint(f\": AUC ROC = {auc_first_letter_surname:.5f}\")\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 3))\n\ndata = pd.DataFrame({\"Fold\": [x + 1 for x in range(n_folds)], \"AUC ROC\": scores})\n_ = sns.lineplot(x=\"Fold\", y=\"AUC ROC\", data=data, ax=ax)\n_ = ax.set_title(\"AUC ROC per Fold\", fontsize=15)\n_ = ax.set_ylabel(\"AUC ROC\")\n_ = ax.set_xlabel(\"Fold #\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:57:59.666156Z","iopub.execute_input":"2024-01-11T11:57:59.667126Z","iopub.status.idle":"2024-01-11T11:58:32.246587Z","shell.execute_reply.started":"2024-01-11T11:57:59.667099Z","shell.execute_reply":"2024-01-11T11:58:32.245832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n\nCalibrationDisplay.from_predictions(train_copy[\"Exited\"], train_oof_preds, n_bins=30, strategy='quantile', ax=ax)\n_ = ax.set_title(\"Prediction Calibration\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:58:32.247620Z","iopub.execute_input":"2024-01-11T11:58:32.248697Z","iopub.status.idle":"2024-01-11T11:58:33.821485Z","shell.execute_reply.started":"2024-01-11T11:58:32.248670Z","shell.execute_reply":"2024-01-11T11:58:33.818991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.4 - Feature: Age ** NumOfProducts\n\nAs discussed in our EDA, our age amplified by the number of products the patron has may provide additional lift. We should test this out as a feature.","metadata":{}},{"cell_type":"code","source":"features = [\n    'Surname', 'Geography', 'Gender', 'CreditScore', \n    'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary'\n]\n\ntrain_copy = train.copy()\n\ntrain_copy[\"Age_NumOfProducts\"] = train_copy[\"Age\"] ** train_copy[\"NumOfProducts\"]\nfeatures.append(\"Age_NumOfProducts\")\n\nfor feature in [\"Surname\", \"Geography\", \"Gender\"]:\n    le = LabelEncoder()\n    train_copy[feature] = le.fit_transform(train_copy[feature])\n    \nn_folds = 5\nskf = StratifiedKFold(n_splits=n_folds, random_state=2023, shuffle=True)\ntrain_oof_preds = np.zeros((train.shape[0],))\nscores = []\n\nfor fold, (train_index, test_index) in enumerate(skf.split(train_copy, train_copy[\"Exited\"])):\n    print(f\"-------> Fold {fold+1} <--------\")\n    x_train, x_valid = pd.DataFrame(train_copy.iloc[train_index]), pd.DataFrame(train_copy.iloc[test_index])\n    y_train, y_valid = train_copy[\"Exited\"].iloc[train_index], train_copy[\"Exited\"].iloc[test_index]\n    \n    x_train_features = pd.DataFrame(x_train[features])\n    x_valid_features = pd.DataFrame(x_valid[features])\n\n    model = LGBMClassifier(\n        random_state=2023,\n        objective=\"binary\",\n        metric=\"auc\",\n        n_jobs=-1,\n        n_estimators=5000,\n        verbose=-1,    \n    )\n    model.fit(\n        x_train_features[features], \n        y_train,\n        eval_set=[(x_valid_features[features], y_valid)],\n        callbacks=[\n            early_stopping(50, verbose=False),\n            log_evaluation(5000),\n        ]\n    )\n    oof_preds = model.predict_proba(x_valid_features[features])[:,1]\n    train_oof_preds[test_index] = oof_preds\n    score = roc_auc_score(y_valid, oof_preds)\n    scores.append(score)\n    print(f\": AUC ROC = {score:.5f}\")\n    \nauc_age_prods = roc_auc_score(train[\"Exited\"], train_oof_preds)\nprint(\"--> Overall results for out of fold predictions\")\nprint(f\": AUC ROC = {auc_age_prods:.5f}\")\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 3))\n\ndata = pd.DataFrame({\"Fold\": [x + 1 for x in range(n_folds)], \"AUC ROC\": scores})\n_ = sns.lineplot(x=\"Fold\", y=\"AUC ROC\", data=data, ax=ax)\n_ = ax.set_title(\"AUC ROC per Fold\", fontsize=15)\n_ = ax.set_ylabel(\"AUC ROC\")\n_ = ax.set_xlabel(\"Fold #\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:58:33.823775Z","iopub.execute_input":"2024-01-11T11:58:33.824365Z","iopub.status.idle":"2024-01-11T11:59:05.443092Z","shell.execute_reply.started":"2024-01-11T11:58:33.824318Z","shell.execute_reply":"2024-01-11T11:59:05.441639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n\nCalibrationDisplay.from_predictions(train_copy[\"Exited\"], train_oof_preds, n_bins=30, strategy='quantile', ax=ax)\n_ = ax.set_title(\"Prediction Calibration\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:59:05.445716Z","iopub.execute_input":"2024-01-11T11:59:05.446271Z","iopub.status.idle":"2024-01-11T11:59:05.884905Z","shell.execute_reply.started":"2024-01-11T11:59:05.446225Z","shell.execute_reply":"2024-01-11T11:59:05.883275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.5 - Feature: Risk Factor Count\n\nOne of the items discussed throughout the EDA was the notion of risk factors. Of these, we came up with the following:\n\n* `Age` > 40\n* `Geography` = Germany\n* `NumOfProducts` > 2\n\nWith that in mind, we can create a risk factor feature that sums up the total number of risk factors.","metadata":{}},{"cell_type":"code","source":"features = [\n    'Surname', 'Geography', 'Gender', 'CreditScore', \n    'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary'\n]\n\ntrain_copy = train.copy()\n\ntrain_copy[\"Risk_Geography\"] = train_copy[\"Geography\"].apply(lambda x: 1 if x == \"Germany\" else 0)\ntrain_copy[\"Risk_Age\"] = train_copy[\"Age\"].apply(lambda x: 1 if x >= 40 else 0)\ntrain_copy[\"Risk_NumOfProducts\"] = train_copy[\"NumOfProducts\"].apply(lambda x: 1 if x > 2 else 0)\ntrain_copy[\"RiskFactors\"] = train_copy[\"Risk_Geography\"] + train_copy[\"Risk_Age\"] + train_copy[\"Risk_NumOfProducts\"]\nfeatures.append(\"RiskFactors\")\n\nfor feature in [\"Surname\", \"Geography\", \"Gender\"]:\n    le = LabelEncoder()\n    train_copy[feature] = le.fit_transform(train_copy[feature])\n    \nn_folds = 5\nskf = StratifiedKFold(n_splits=n_folds, random_state=2023, shuffle=True)\ntrain_oof_preds = np.zeros((train.shape[0],))\nscores = []\n\nfor fold, (train_index, test_index) in enumerate(skf.split(train_copy, train_copy[\"Exited\"])):\n    print(f\"-------> Fold {fold+1} <--------\")\n    x_train, x_valid = pd.DataFrame(train_copy.iloc[train_index]), pd.DataFrame(train_copy.iloc[test_index])\n    y_train, y_valid = train_copy[\"Exited\"].iloc[train_index], train_copy[\"Exited\"].iloc[test_index]\n    \n    x_train_features = pd.DataFrame(x_train[features])\n    x_valid_features = pd.DataFrame(x_valid[features])\n\n    model = LGBMClassifier(\n        random_state=2023,\n        objective=\"binary\",\n        metric=\"auc\",\n        n_jobs=-1,\n        n_estimators=5000,\n        verbose=-1,    \n    )\n    model.fit(\n        x_train_features[features], \n        y_train,\n        eval_set=[(x_valid_features[features], y_valid)],\n        callbacks=[\n            early_stopping(50, verbose=False),\n            log_evaluation(5000),\n        ]\n    )\n    oof_preds = model.predict_proba(x_valid_features[features])[:,1]\n    train_oof_preds[test_index] = oof_preds\n    score = roc_auc_score(y_valid, oof_preds)\n    scores.append(score)\n    print(f\": AUC ROC = {score:.5f}\")\n    \nauc_risk_factors = roc_auc_score(train[\"Exited\"], train_oof_preds)\nprint(\"--> Overall results for out of fold predictions\")\nprint(f\": AUC ROC = {auc_risk_factors:.5f}\")\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 3))\n\ndata = pd.DataFrame({\"Fold\": [x + 1 for x in range(n_folds)], \"AUC ROC\": scores})\n_ = sns.lineplot(x=\"Fold\", y=\"AUC ROC\", data=data, ax=ax)\n_ = ax.set_title(\"AUC ROC per Fold\", fontsize=15)\n_ = ax.set_ylabel(\"AUC ROC\")\n_ = ax.set_xlabel(\"Fold #\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:59:05.886368Z","iopub.execute_input":"2024-01-11T11:59:05.886687Z","iopub.status.idle":"2024-01-11T11:59:38.152266Z","shell.execute_reply.started":"2024-01-11T11:59:05.886661Z","shell.execute_reply":"2024-01-11T11:59:38.149971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n\nCalibrationDisplay.from_predictions(train_copy[\"Exited\"], train_oof_preds, n_bins=30, strategy='quantile', ax=ax)\n_ = ax.set_title(\"Prediction Calibration\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:59:38.154936Z","iopub.execute_input":"2024-01-11T11:59:38.156619Z","iopub.status.idle":"2024-01-11T11:59:38.580843Z","shell.execute_reply.started":"2024-01-11T11:59:38.156555Z","shell.execute_reply":"2024-01-11T11:59:38.579133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.6 - Target Encoding\n\nWe have a number of encoding options when it comes to encoding our categorical data into a form our classifiers can use. While label encoding is quick, we have other options such as target encoding. With target encoding, we calculate the mean value of the categorical value with respect to the target. In other words, we calculate the prior probability of the categorical value being associated with `Exited`. This method does have a drawback: it leaks information about our target variable into the features themselves. However, given that it is computationally inexpensive, we should explore it as an option.","metadata":{}},{"cell_type":"code","source":"from category_encoders.target_encoder import TargetEncoder\n\nfeatures = [\n    'Surname', 'Geography', 'Gender', 'CreditScore', \n    'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary'\n]\n\ntrain_copy = train.copy()\n\nfor feature in [\"Surname\", \"Geography\", \"Gender\"]:\n    te = TargetEncoder()\n    train_copy[feature] = te.fit_transform(train_copy[feature], train_copy[\"Exited\"])\n    \nn_folds = 5\nskf = StratifiedKFold(n_splits=n_folds, random_state=2023, shuffle=True)\ntrain_oof_preds = np.zeros((train.shape[0],))\nscores = []\n\nfor fold, (train_index, test_index) in enumerate(skf.split(train_copy, train_copy[\"Exited\"])):\n    print(f\"-------> Fold {fold+1} <--------\")\n    x_train, x_valid = pd.DataFrame(train_copy.iloc[train_index]), pd.DataFrame(train_copy.iloc[test_index])\n    y_train, y_valid = train_copy[\"Exited\"].iloc[train_index], train_copy[\"Exited\"].iloc[test_index]\n    \n    x_train_features = pd.DataFrame(x_train[features])\n    x_valid_features = pd.DataFrame(x_valid[features])\n\n    model = LGBMClassifier(\n        random_state=2023,\n        objective=\"binary\",\n        metric=\"auc\",\n        n_jobs=-1,\n        n_estimators=5000,\n        verbose=-1,    \n    )\n    model.fit(\n        x_train_features[features], \n        y_train,\n        eval_set=[(x_valid_features[features], y_valid)],\n        callbacks=[\n            early_stopping(50, verbose=False),\n            log_evaluation(5000),\n        ]\n    )\n    oof_preds = model.predict_proba(x_valid_features[features])[:,1]\n    train_oof_preds[test_index] = oof_preds\n    score = roc_auc_score(y_valid, oof_preds)\n    scores.append(score)\n    print(f\": AUC ROC = {score:.5f}\")\n    \nauc_target_encode = roc_auc_score(train[\"Exited\"], train_oof_preds)\nprint(\"--> Overall results for out of fold predictions\")\nprint(f\": AUC ROC = {auc_target_encode:.5f}\")\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 3))\n\ndata = pd.DataFrame({\"Fold\": [x + 1 for x in range(n_folds)], \"AUC ROC\": scores})\n_ = sns.lineplot(x=\"Fold\", y=\"AUC ROC\", data=data, ax=ax)\n_ = ax.set_title(\"AUC ROC per Fold\", fontsize=15)\n_ = ax.set_ylabel(\"AUC ROC\")\n_ = ax.set_xlabel(\"Fold #\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T11:59:38.582472Z","iopub.execute_input":"2024-01-11T11:59:38.582803Z","iopub.status.idle":"2024-01-11T12:00:11.371402Z","shell.execute_reply.started":"2024-01-11T11:59:38.582777Z","shell.execute_reply":"2024-01-11T12:00:11.368608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n\nCalibrationDisplay.from_predictions(train_copy[\"Exited\"], train_oof_preds, n_bins=30, strategy='quantile', ax=ax)\n_ = ax.set_title(\"Prediction Calibration\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T12:00:11.378684Z","iopub.execute_input":"2024-01-11T12:00:11.379552Z","iopub.status.idle":"2024-01-11T12:00:11.866638Z","shell.execute_reply.started":"2024-01-11T12:00:11.379446Z","shell.execute_reply":"2024-01-11T12:00:11.865028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.7 - All Beneficial Features\n\nWith all our beneficial features turned on, let's run a check to ensure we see lift.","metadata":{}},{"cell_type":"code","source":"features = [\n    'Surname', 'Geography', 'Gender', 'CreditScore', \n    'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary'\n]\n\ntrain_copy = train.copy()\n\ntrain_copy[\"Surname_First_Letter\"] = train_copy[\"Surname\"].apply(lambda x: x[0])\nfeatures.append(\"Surname_First_Letter\")\n\nfor feature in [\"Surname\", \"Geography\", \"Gender\", \"Surname_First_Letter\"]:\n    te = TargetEncoder()\n    train_copy[feature] = te.fit_transform(train_copy[feature], train_copy[\"Exited\"])\n\ntrain_copy[\"Risk_Geography\"] = train_copy[\"Geography\"].apply(lambda x: 1 if x == \"Germany\" else 0)\ntrain_copy[\"Risk_Age\"] = train_copy[\"Age\"].apply(lambda x: 1 if x >= 40 else 0)\ntrain_copy[\"Risk_NumOfProducts\"] = train_copy[\"NumOfProducts\"].apply(lambda x: 1 if x >= 2 else 0)\ntrain_copy[\"RiskFactors\"] = train_copy[\"Risk_Geography\"] + train_copy[\"Risk_Age\"] + train_copy[\"Risk_NumOfProducts\"]\nfeatures.append(\"RiskFactors\")\n\nn_folds = 5\nskf = StratifiedKFold(n_splits=n_folds, random_state=2023, shuffle=True)\ntrain_oof_preds = np.zeros((train.shape[0],))\nscores = []\n\nfor fold, (train_index, test_index) in enumerate(skf.split(train_copy, train_copy[\"Exited\"])):\n    print(f\"-------> Fold {fold+1} <--------\")\n    x_train, x_valid = pd.DataFrame(train_copy.iloc[train_index]), pd.DataFrame(train_copy.iloc[test_index])\n    y_train, y_valid = train_copy[\"Exited\"].iloc[train_index], train_copy[\"Exited\"].iloc[test_index]\n    \n    x_train_features = pd.DataFrame(x_train[features])\n    x_valid_features = pd.DataFrame(x_valid[features])\n\n    model = LGBMClassifier(\n        random_state=2023,\n        objective=\"binary\",\n        metric=\"auc\",\n        n_jobs=-1,\n        n_estimators=5000,\n        verbose=-1,    \n    )\n    model.fit(\n        x_train_features[features], \n        y_train,\n        eval_set=[(x_valid_features[features], y_valid)],\n        callbacks=[\n            early_stopping(50, verbose=False),\n            log_evaluation(5000),\n        ]\n    )\n    oof_preds = model.predict_proba(x_valid_features[features])[:,1]\n    train_oof_preds[test_index] = oof_preds\n    score = roc_auc_score(y_valid, oof_preds)\n    scores.append(score)\n    print(f\": AUC ROC = {score:.5f}\")\n    \nauc_all_good = roc_auc_score(train[\"Exited\"], train_oof_preds)\nprint(\"--> Overall results for out of fold predictions\")\nprint(f\": AUC ROC = {auc_all_good:.5f}\")\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 3))\n\ndata = pd.DataFrame({\"Fold\": [x + 1 for x in range(n_folds)], \"AUC ROC\": scores})\n_ = sns.lineplot(x=\"Fold\", y=\"AUC ROC\", data=data, ax=ax)\n_ = ax.set_title(\"AUC ROC per Fold\", fontsize=15)\n_ = ax.set_ylabel(\"AUC ROC\")\n_ = ax.set_xlabel(\"Fold #\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T12:00:11.869121Z","iopub.execute_input":"2024-01-11T12:00:11.869578Z","iopub.status.idle":"2024-01-11T12:00:43.604622Z","shell.execute_reply.started":"2024-01-11T12:00:11.869549Z","shell.execute_reply":"2024-01-11T12:00:43.601913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n\nCalibrationDisplay.from_predictions(train_copy[\"Exited\"], train_oof_preds, n_bins=30, strategy='quantile', ax=ax)\n_ = ax.set_title(\"Prediction Calibration\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T12:00:43.607084Z","iopub.execute_input":"2024-01-11T12:00:43.607614Z","iopub.status.idle":"2024-01-11T12:00:44.124124Z","shell.execute_reply.started":"2024-01-11T12:00:43.607573Z","shell.execute_reply":"2024-01-11T12:00:44.122647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.x - Model Comparisons\n\nLet's compare how the different approaches perform.","metadata":{}},{"cell_type":"code","source":"bar, ax = plt.subplots(figsize=(20, 10))\nax = sns.barplot(\n    x=[\"Baseline\", \"Categorical On\", \"First Letter Surname\", \"Age ** Products\", \"Risk Factors\", \"Target Encoding\", \"All Good\"],\n    y=[\n        auc_baseline,\n        auc_category,\n        auc_first_letter_surname,\n        auc_age_prods,\n        auc_risk_factors,\n        auc_target_encode,\n        auc_all_good,\n    ]\n)\n_ = ax.axhline(y=auc_baseline, color='r', linestyle='--')\n_ = ax.set_title(\"AUC ROC (Higher is Better)\", fontsize=15)\n_ = ax.set_xlabel(\"\")\n_ = ax.set_ylabel(\"AUC ROC\")\n_ = ax.set_ylim([0.89, 0.90])\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(\n        x=p.get_x()+(p.get_width()/2),\n        y=height,\n        s=\"{:.5f}\".format(height),\n        ha=\"center\"\n    )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-11T12:00:44.126526Z","iopub.execute_input":"2024-01-11T12:00:44.127021Z","iopub.status.idle":"2024-01-11T12:00:44.575629Z","shell.execute_reply.started":"2024-01-11T12:00:44.126989Z","shell.execute_reply":"2024-01-11T12:00:44.574011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4 - Conclusions\n\nThere are some interesting points we discovered in the datasets:\n\n* The dataset is highly skewed to the negative class.\n* With a large number of samples to learn from, our results may be more stable between local CV, public LB, and private LB. \n* No null values appear in the training or testing datasets.\n* Duplicate columns exist in the dataset, and identical entries have different mappings to `Exited`.\n* Adversarial validation suggests that the training dataset and the testing dataset are very similar.\n* Adversarial validation suggests there are some easy-to-spot differences between the original dataset and the competition dataset.\n    * Caution should be used when mixing data.\n    * The `Surname` categorical column appears to be a distinguishing factor between datasets.\n* P-value testing suggests there are no features that are likely candidates for removal.\n* For the `Surname` feature:\n    * Different types of categorical encoding may make a difference to our classifier.\n    * Using only the first letter in the surname as a feature may provide lift.\n* For the `Geography` feature:\n    * People leaving occurs more in Germany than other geographic locations - this may be useful to generate a _risk factor_ feature.\n* For the `Gender` feature:\n    * Males are far less likely to exit the bank.\n    * This holds true in both `Spain` and `France`.\n    * Males in `Germany` are much more likely to exit when compared to other geographies.\n    * The `Gender` feature in `Spain` and `France` may be useful to generate a _mitigating factors_ feature.\n* For the `Age` feature:\n    * Age is a strong indicator for our target.\n    * We may be able to create an additional risk factor based on age, as ages over 40 are more likely to exit.\n    * Age combined with number of products may provide lift as an additional feature.\n","metadata":{}},{"cell_type":"markdown","source":"## More to Come...","metadata":{}},{"cell_type":"code","source":"features_test = [\n    'Surname', 'Geography', 'Gender', 'CreditScore', \n    'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n    'IsActiveMember', 'EstimatedSalary'\n]\n\n# Copy the test data\ntest_copy = test.copy()\n\n# Target encode categorical features for test data\nfor feature in [\"Surname\", \"Geography\", \"Gender\", \"Surname_First_Letter\"]:\n    te = TargetEncoder()\n    test_copy[feature] = te.fit_transform(test_copy[feature], train_copy[\"Exited\"])\n\n# Additional feature engineering for test data\ntest_copy[\"Risk_Geography\"] = test_copy[\"Geography\"].apply(lambda x: 1 if x == \"Germany\" else 0)\ntest_copy[\"Risk_Age\"] = test_copy[\"Age\"].apply(lambda x: 1 if x >= 40 else 0)\ntest_copy[\"Risk_NumOfProducts\"] = test_copy[\"NumOfProducts\"].apply(lambda x: 1 if x >= 2 else 0)\ntest_copy[\"RiskFactors\"] = test_copy[\"Risk_Geography\"] + test_copy[\"Risk_Age\"] + test_copy[\"Risk_NumOfProducts\"]\nfeatures_test.append(\"RiskFactors\")\n\n# Ensure the test data contains all the required features\ntest_features = pd.DataFrame(test_copy[features_test])\n\n# Predict on the test set\npreds_test = model.predict_proba(test_features)[:, 1]\n\n# Save test predictions to file\noutput = pd.DataFrame({'id': test.index, 'Exited': preds_test})\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T12:24:42.666047Z","iopub.execute_input":"2024-01-11T12:24:42.668647Z","iopub.status.idle":"2024-01-11T12:24:44.732297Z","shell.execute_reply.started":"2024-01-11T12:24:42.668527Z","shell.execute_reply":"2024-01-11T12:24:44.730572Z"},"trusted":true},"execution_count":null,"outputs":[]}]}